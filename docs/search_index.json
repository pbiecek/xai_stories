[
["index.html", "XAI Stories Preface", " XAI Stories 2020-05-18 Preface This book is the result of a student projects for Interpretable Machine Learning course at the University of Warsaw and the Warsaw University of Technology. Each team has prepared one case study for selected XAI technique. This project is inspired by a fantastic book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich. We used the LIML project as the cornerstone for this repository. The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using github repository. Cover by kozaka93. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],
["foreword.html", "Foreword 0.1 Why? 0.2 What? 0.3 How?", " Foreword Author: Przemyslaw Biecek (Warsaw University of Technology and University of Warsaw) 0.1 Why? Machine learning has a number of applications. Very often, however, machine learning predictive models are treated as black boxes which can be automatically trained without worrying about the domain in which they are used. This opaqueness rises many risks that are difficult to foresee during the model building process. Such as the model’s declining performance due to the data drift, poor performance on the out-of-domain problems or unfair biased behaviour learned on historical data. The growing list of examples where black boxes fail spectacularly has led to an increased interest in XAI methods. Such methods allow to x-ray black boxes models for more detailed analysis on the local or global level. According to Gartner Hype Cycle for Emerging Technologies in 2019 Explainable AI is on the verge of Innovation trigger and Peak of inflated expectations. It is a technology with a very high potential, which is talked about a lot in the media and which heats up the imagination as strongly as AI. In the literature there are many articles arguing the need to use XAI methods as well as many ideas for new methods from the XAI family. However, it is much more difficult to find examples of successful implementations of XAI methods that have improved the business. Missing elements are case studies of actual use of XAI methods in machine learning problems. Such case studies would allow a better understanding of what is possible today and what is not possible using XAI methods. 0.2 What? This ebook collects examples of the use of different methods from the XAI family for different real-world predictive problems. In the following chapters, we show example applications of different XAI techniques to problems based on real-world public dataset. These examples are called XAI stories and like every good story, each one has a structure. It starts with a description of the predictive problem, goes on to describe the proposed model or models. The models are x-rays using XAI techniques to finish the chapter with a point. 0.3 How? For XAI stories to be credible they need not only a strong predictive model, but also business validation of the proposed modeling and explanation approach. Each group of students got two mentors from McKinsey’s Data Science department. The mentors, together with the students, searched for strengths and weaknesses of XAI applications in specific problems. TODO: write more about this collaboration "],
["story-compas.html", "Chapter 1 Story COMPAS: recidivism reloaded 1.1 Introduction 1.2 Models 1.3 Explanations 1.4 Summary and conclusions", " Chapter 1 Story COMPAS: recidivism reloaded Authors: Łukasz Grad (University of Warsaw), Katarzyna Koprowska (Warsaw University of Technology), Jakub Kozak (SGH Warsaw School of Economics) Mentors: Michał Miktus (McKinsey) 1.1 Introduction The study concerns the COMPAS algorithm, which stands for Correctional Offender Management Profiling for Alternative Sanctions, created by a for-profit company Northpointe. COMPAS is a widely popular commercial algorithm used by judges and parole officers for scoring a criminal defendant’s likelihood of recidivism. It was designed to help judges identify potentially more dangerous individuals (those with higher scores) and award them with longer sentences. It is easy to notice that COMPAS results may have very serious consequences for the lives of many people in the United States. yet the algorithm is (because of its proprietary nature) still a black-box for the wide audience – meaning that we cannot easily identify which factors did it take into account when classifying an individual as a person with a high or low likelihood of reoffending. It is natural that many questions have arised about the fairness of such an algorithm, especially as the fairness required to be defined in advance. 1.1.1 Previous work One of the first and the most known investigators wanting to validate COMPAS results was ProPublica group. They have provoked a vigorous discussion about the fairness of black-box models with their 2016 study, which attempted to reconstruct COMPAS methodology. They have collected criminal records from Broward County FL for several thousand people, as well as reports about their offenses in a two-year follow-up period. The overall accuracy of ProPublica’s reconstructed model was only 61%, but their main discovery was a racial bias in favour of Caucasian defendants over those of African-American origins. According to the study, black defendants were particularly likely to be falsely flagged as future criminals almost twice as often as white ones, who were also more often mislabeled as low risk. The researchers believed that this disparity cannot be explained by either defendant’s prior crimes, type of crimes, gender or age. The ProPublica study was, however, criticized for its flawed methodology. One of the critics was Northpointe, the creator of the COMPAS algorithm, who defended the accuracy of its test, because the results from ProPublica do not accurately reflect their model. After ProPublica’s publication, confusion and doubts, whether COMPAS should still be relied on, began to appear among researchers, which led several of them to propose their own validations of the algorithm, based on data provided by ProPublica. One of the most reliable work seems to be the 2019 study “The age of secrecy and unfairness in recidivism prediction” by Rudin et al., which verifies the analysis conducted by ProPublica, indicating cases where the results given by COMPAS may be non-intuitive and possible explanations. The authors believe that ProPublica has drawn conclusions from incorrect assumptions and lack of knowledge of all data: for instance, they assumed linearity in age (as stated in official COMPAS documentation from Northpointe), which appeared to be untrue; they also did not have access to the answers from questionnaires given to defendants (also introduced by the Northpointe as predictors for their model), which can be highly correlated with race and shift the outcome. Disproving ProPublica’s study was not, however, the main objective of Rudin et al. They described what they believed was the real problem of COMPAS: its proprietary nature, which, along with over a hundred of (manually-entered) variables collected from questionnaires not accessible to anyone but Northpointe, does not allow to identify data entry errors, data integration errors, missing data and other types of inaccuracies. The researchers even identified some individuals with a rich criminal history and low probability of recidivism given by model, highly suggesting that their scores were based on flawed data. The main conclusion of this analysis was to replace black-box machine learning models by interpretable models, which, as Rudin et al. suggested, can be equally accurate for predicting recidivism. We believe, however, that we are capable of achieving better accuracy results than the aforementioned researchers and that, thanks to the explainability techniques, a trade-off between accuracy and interpretability is no longer unavoidable. 1.2 Models Our goal was to predict the recidivism and violent recidivism in a two-year follow-up period. Therefore, we modeled both problems as binary classification tasks. Data we used for modelling was largely derived from the “The age of unfairness” study dataset combined with several factors extracted from the raw Broward County FL database used by ProPublica for their analysis. In our model, we analyzed data from 5727 subjects from Broward County FL, whose likelihood of recidivism and violent recidivism we were trying to predict. We have used both 29 variables: personal (6) including current age, age at first offence, sex, race, marital status, custody status, criminal involvement (20) consisting of number and types of previous charges and arrests, as well as those leading to COMPAS screening and history of non-compliance (3) concerning behaviour while on probation. 1.2.1 Recidivism In order to find the best model, we have tried different approaches: Extreme Gradient Boosting (XGBoost), LASSO Logistic Regression and Random Forest. We focused on tuning the XGBoost model, as it provides state-of-the-art results on many tabular datasets, and wanted to compare it with a white box logistic regression model along with out-of-the-box Random Forest. Comparison with a linear model can give us insights on whether the relationships between recidivism and given predictors is highly nonlinear or not. On the other hand, comparison with Random Forest model will reveal how important is careful parameter tuning for this task. 1.2.1.1 XGBoost Since the volume of our datasets is not substantial, we performed XGBoost tuning with exhaustive grid search method in a coarse-to-fine manner. With a coarse parameter search we assessed the relative importance of each parameter and narrowed down its potential range of values. Next, with a much finer search we obtained out final sets of parameters. As a metric during parameter tuning we used AUC on the full training set with out-of-fold scores within a 5-fold CV scheme. In our analysis, we used xgboost package in R. Model Tree Number Max Depth Eta Colsample Subsample Min Child Weight Alpha Lambda XGB 12 3 0.3 1 0.8 3 1 1 XGB Violent 19 3 0.3 0.8 0.8 10 1 0 Selected final parameter sets for recidivism and violent recidivism XGBoost models 1.2.1.2 LASSO regression Since we had strong suspicions about collinearity between predictors, we used Logistic Regression with L1 penalty (LASSO) as our linear model of choice. The penalty term, similarily to XGBoost, was tuned with 5-fold CV on the training set. We utilized a well known glmnet package in R to efficiently find optimal penalty terms using the Lasso path. Final models had \\(17\\) and \\(28\\) non-zero weights, for recidivism and violent recidivism models. Below we present the Lasso path for recidivism model. Lasso path for recidivism model. On top we see the number of non-zero weights. We can see the optimal penalty term chosen close to \\(e^{-5}\\). Best model had 17 non-zero weights, although models with less than 10 chosen predictors also perform well 1.2.1.3 Random Forest As for the Random Forest model, we relied on the default implementation in RandomForest R package, without any finetuning. We set the number of trees to 50, seeing that the chosen XGBoost models were also sparse in tree count. Figure above presents ROC curve for three recidivism classifiers. We decidecied to compare our models using a receiver operating characteristic or ROC curve in short. As it turned out, the most accurate algorithm was XGBoost with AUC of 0.72. Figure above presents ROC curve for three violent recidivism classifiers. We did similar calculations for violent recidivism and again the best results gave us XGBoost. In both cases, ROC AUC was 0.72, which is much more than ProPublika score and also significantly more than COMPAS. 1.3 Explanations 1.3.1 Model specific In order to identify the effect of features on prediction we used Permutation Variable Importance. Not surprisingly, the most relevant variable turned out to be number of previous arrests. Another factor that is greatly affecting prediction is age – younger people tend to be much more likely to commit crime again. It is tightly followed by the number of previous misdemeanors, suggesting that people commiting lesser crimes are more likely to do it again. Last of the important characteristics is the age of first offense, indicating that people whose criminal history begins in their teens are more likely to re-offend. Similarly to recidivism modeling, the number of previous arrests and current age are also the most important factors in predicting violent recidivism. The deciding factor is, however, the number of previous violent charges – people committing violent crimes tend to be more likely to proceed to do it in the future. The impact of the age at first offense is lower comparing to the recidivism model. Our model predicted African-Americans to be, on average, more likely to re-offend than individuals of any other race. 1.3.2 Instance specific In order to identify, whether it is not caused by other factors correlated with race, we have performed an instance level analysis using Ceteris Paribus on observations with the most accurate predictions. Neither is our model – but it has been fitted on real world data full of systematic bias, making it unfair towards African-Americans,as Ceteris Paribus freezes all the other factors. It is very surprising that the COMPAS itself seems to (for some of the subjects) favorite black people over other races, as it can be seen on the picture. Another question arises whether we do not have a gender bias, because both models - ours and COMPAS - seem to drastically change their predictions for male subjects when controlling for other factors. In our next steps we would like to investigate other factors affecting predictions and check whether biases other than racial occur as well. 1.4 Summary and conclusions One may ask: what is the big deal? If data shows that certain individuals are more likely to be classified as reoffending, then our models should include that information. We do not know, however, how existing racial bias influenced the actual data: what if white people were simply less often arrested, had fewer charges and more let go with a warning? This scenario is not so difficult to imagine. As data scientists it is our social duty to make sure we do not propagate any existing biases and do our best to eliminate them. "],
["story-house-sale-prices.html", "Chapter 2 Story House Sale Prices: eXplainable predictions for house sale 2.1 Introduction 2.2 Data preperation 2.3 Model 2.4 Explanations 2.5 Summary and conclusions", " Chapter 2 Story House Sale Prices: eXplainable predictions for house sale Authors: Piotr Grązka (SGH Warsaw School of Economics), Anna Kozak (Warsaw University of Technology), Paweł Wicherek (Warsaw University of Technology) Mentors: Mateusz Zawisza (McKinsey &amp; Company), Adam Zmaczyński (McKinsey &amp; Company) ‘’That’s all your house is: it’s a place to keep your stuff while you go out and get more stuff.’’ George Carlin 2.1 Introduction Everybody needs a roof over their heads. It can be a house, villa, or a flat. Everybody, at some point in life, faces a choice if to buy a house. If so, which one. And why they are so expensive? The topic of real estate is not only the topic you just have to deal with. It can also be very interesting. There are plenty of TV Shows, for instance, Property Brothers, of which plot is based on examples of people buying and renovating houses. This particular one is the most famous in the world and has been running already for almost a decade. For many people houses are also products to buy and sell with income. Regardless of motives of buy/sell real estate, both sides agree to a price. It is always good to know, how much it is worth, what’s the fair/true value. And, maybe it’s even more important, why the price is like that, what has an influence on it. In this work we want to try to find an answer to both questions with a stronger emphasis on the second one. This paper is intended to be a complete use case on how to deal with the regression problem for Data Scientists. Let’s start with a couple of questions that will allow us to understand and define the problems The seller does not know how to increase the cost of the apartment so that the investment outlay is lower than the added value (e.g. building a pool will increase the price and renovating the bathroom is not worth it). The seller does not know how much to sell the apartment for (he makes an offer on the portal and does not know if the price is fair). The buyer does not know how much the apartment is worth (as above, whether the price is fair). Commercial problem: Auction services do not have tools to support sellers/buyers. These are just some of the questions we can ask. As a definition of our problem, we have set ourselves a property valuation, and thorough explanations we will try to get an answer depending on the position we choose. We have divided our work into several stages, below we present a diagram (Figure 2.1) with a step plan. It allowed us to plan our work, and now we will use it to tell you what we did. FIGURE 2.1: A scheme with a step-by-step plan that allowed us to plan our work. We started our work with a literature review. Many papers show a comparison of hedonistic models (linear regression) and machine learning models. Below is a graph with results achieved by models that can be interpreted by design (hedonistic) and black box model (ANN) based on the article (Selim 2009). FIGURE 2.2: Comparison of the performance of hedonic regression model and ANN(artificial neural networks). We can conclude from Figure 2.2 that we reduce the interpretability to an increase in the quality of model fitting. The next point was data analysis, we work on a dataset which contains house sale prices for King County, which includes Seattle. It is include homes sold between May 2014 and May 2015. Data available on kaggle and openml. We have analyzed the data, more. Data contains 19 house features plus the price and the id columns, along with 21613 observations. Description of variables in the dataset. Variable Description id unique ID for each home sold date date of the home sale price price of each home sold bedrooms number of bedrooms bathrooms number of bathrooms, where .5 accounts for a room with a toilet but no shower sqft_living square footage of the apartments interior living space sqft_lot square footage of the land space floors number of floors waterfront apartment was overlooking the waterfront or not view how good the view of the property was condition condition of the apartment grade level of construction and design sqft_above the square footage of the interior housing space that is above ground level sqft_basement the square footage of the interior housing space that is below ground level yr_built the year the house was initially built yr_renovated the year of the house’s last renovation zipcode zipcode area lat lattitude long longitude sqft_living15 the square footage of interior housing living space for the nearest 15 neighbors sqft_lot15 the square footage of the land lots of the nearest 15 neighbors We have collected methods to evaluate the performance of the regression model, we decided to use RMSE (root mean square errors). Based on the literature we decided to test the following models: linear regression fixed effects model random effects model decision tree random forest gradient boosting xgboost Another idea of how to enrich our solution was to add external data. The location of the property can significantly affect the price, so we also took into account the distance from public transport and the number of cultural facilities within a kilometer radius. 2.2 Data preperation Original data from kaggle is in good quality at the start, but we need to preprocess it to suit our needs. In this section we want to describe how we prepared ready-to-use csvs of the train and test data (train.csv, test.csv). This includes variable transformations, joining external data, records processing. Firstly, we discovered that there are houses that were sold twice, and one house that was sold three times. One can assume, that it was bought, renovated and then sold for more. But they have the same explanatory variables, no changes. Each pair (triple) of houses we decide to aggregate into a single row with averaging the price. Secondly, we decided to add external data. Why? We believe most variables describe the house properly. What we are missing is some spatial information. Except zip code, we have longitude and latitude – good, that’d be helpful – but we want to explain it more. Why some locations tend to be more expensive? Maybe it’s because of public transport availability. That’s why we decided to add a variable describing a distance to the nearest bus/subway stop. Data source can be found here. There might be also another reason. Maybe some houses are more expensive, because of some interesting places around, like museums, galleries or fine restaurants. We’ll call them cultural places. Those places are not only standalone facilities, but they are connected to other infrastructure, which generally should increase the price. This time we decided to look in the neighborhood: we search for a number of such places in arbitrarily chosen 1km range. Data obtained from here. FIGURE 2.3: Spatial external data. Stops on the left and cultural places on the right. Both of these external data we visualize at the figure 2.3. At this point it is also worth a comment. Let us begin with the latter. Most of them, not all, are concentrated in the city centre. So this column also tells some story how much of the city center does this house have. Since not all of those places are in the city center, then those other points should reflect some local centers. Public transport stops are very dense in the city center, so they even cover house dots on the plot. Outside of the city center, one can also notice buses routes. It is quite clear, that not every house has good connection to public transport. That’d force parents to drive their children where they need to. For some people it would be kind of an obstacle, so they would rather more sceptical about that particular house, thus reducing demand of that house, which should lower the price. We also did several variable transformations. Since for authors it is hard to think in square feet, we translate it into square meters. zipcode and waterfront are saved as factors. Further, for us it’s easier for us to interpret the age of a building rather than a year when it was built. We also know, if and when a house has been renovated. We can also analyze relative age, that is the time which has past since last renovation (variable since_renovated). Here we also log the price, since we want to work on a relative scale. The last step is division the data into train and test samples randomly with ratio 70/30. Ready script for processing the original data from kaggle is on the github. For spatial data analysis, we were using a short script geofast. Distances between two arbitrary points on earth can be obtained from the (Esmukov 2020) package for Python. However, general and precise formula is computationally expensive. With a simple trick, it can be adjusted to our case without losing almost any precision. This can be done by providing distances to measure are not exceeding several hundred kilometers. The original idea was published here. 2.3 Model Inspired by literature, we started our analysis by building a white-box linear model. The aim was not to build an ideal model, but rather to get an insight into the relationships in the data and to have a point of reference for more complex models performance and interpretations. Based on economic interpretation, we chose logarithm of price as the dependent variable. The continuous variables characterizing areas were also logarithmized. Variables referring to latitude and longitude were omitted since they are unlikely to have a linear effect on price. Instead, zip codes were used to model geographical effects. To avoid collinearity with years since renovation variable, age variable was omitted. This model ignores information about house location almost completely (part of this information is carried only by variables such as distance to the nearest bus stop). It assumes that the price of a house in the city centre is driven by the same effects as the price of a house in the suburbs. One could argue that this assumption is almost never true, and instead, the data should be modelled by the panel or spatial techniques. Taking this into consideration, we developed a second model that allows for prices of houses belonging in different zip codes to have their own intercept. The underlying assumption is that the closest area of houses will have a fixed effect on their price. It is the same model as before but with new binary variables (in number equal to a number of zip codes - in our case: 70) indicating whether a house belongs to a certain zipcode or not. A neighbourhood, however, can also have an impact on the effect of the particular variables on price. Increasing an area of a house can have a different impact on its price, depending whether the house is located in the city centre, or in the suburbs. The final model takes this argument into consideration, allowing for houses in different zip codes to have different slopes coefficients for certain variables. Technically, those differences are modelled as random deviations from the general effects. The choice of variables that include random effect was made arbitrarily, to reduce the complexity of the problem. Only variables for which a random deviation of their impact could be easily interpreted were chosen (e. g. view importance can vary between geographical locations, since the views themselves are different). We are aware that one could find different approaches that could better fit the data. However, since our scope was not to maximize fit, and no further analysis yielded considerably different results, we decided to limit the consideration to the three models presented. Estimated coefficients are presented in the table below. Since the dependent variable is the logarithm of price, the coefficients have an interpretation of a percentage change. Looking at the estimates, a couple of general observations can be made. Firstly, there are no major differences in the coefficients between models, especially between (2) and (3). This tells us that the models can be considered stable. Basing on RMSE, we should note a significant improvement of fit resulting from introducing zip code-based fixed effects in the model (2). A slight, but noticeable improvement was also made by allowing for coefficients to have a random effects in the model (3). These observations suggest that our hypothesis about neighbourhood having a significant impact on a house price, but also about particular variables’ effect on price, was reasonable. Estimated coefficients for linear models. Model Log-loglinear (1) Zipcode-based intercepts (2) Random effects (3) (Intercept) 8.1531 8.8260(RE) bedrooms -0.0304 -0.0096 -0.0079 bathrooms 0.0579 0.0409 0.0379 floors 0.0293 -0.0339 -0.0186 waterfront 0.4605 0.5208 0.4641(RE) view 0.0559 0.0609 0.0543(RE) condition 0.0495 0.0557 0.0586 grade 0.1886 0.0885 0.0876 dist_stop_km -0.0106 -0.0041 -0.0047 ncult 0.0134 0.0016 0.0026 since_renovated 0.0038 -0.0002 -0.0007 m_2_lot_log 0.0043 0.0727 0.0841(RE) m_2_above_log 0.3257 0.4099 0.3844(RE) m_2_basement_log 0.0446 0.0270 0.0271 m_2_living_15_log 0.2877 0.1729 0.1566(RE) m_2_lot_15_log -0.0314 -0.0126 (RE) RMSE(train) 0.3054 0.1797 0.1707 RMSE(test) 0.3061 0.1801 0.1762 Estimated coefficients can be interpreted rather intuitively. It is not surprising that the area of the house has a considerable positive impact on price. The same can be said about variables characterizing the quality of the property (grade, condition) and its surroundings (view, waterfront). Especially we can note how important access to the waterfront is in Seattle, that is related to the city geography. Since it is more convenient to have more bathrooms in the house, bathrooms coefficient is also positive. Higher distance to bus stop results in a slight decrease in house value, which again is correct with intuitive expectations. Number of floors, which impact is also negative, can be associated with lower convenience. One may be wondering why years passing since the last renovation do not have a great impact on price. It can be explained by the fact that we already include variables characterizing the condition of the building, and with those being constant – years become not much more than number. To further assess the model performance, we may take a look into the residual density (Figure2.4). Most of the residuals being in (-0,5; 0,5) range means our model makes the biggest error of approximate -40% and +66%. FIGURE 2.4: Residuals density for random effects (3) model. Plotting residuals against explained variable gives us some more insight into what happened in the model. FIGURE 2.5: Plot the residual for the random effect model. The residuals correspond to the difference between the value of the target variable and the prediction from the model. On the x-axis we have the value of the target variable (price logarithm), on the y-axis of the residual, each point corresponds to the observation from the test set. We can note a lot of points being in the lower left part of the plot. This tells us that the model overestimated prices of low-valued houses. The residuals cannot be considered normally distributed. Overall, the model gave us numerous insights and reasonable predictions. It is, however, far from being perfect. The analysis of residuals clearly suggests that there are some other effects in the data that we failed to model with linear regression. These may result from too few variables being taken into consideration, or, what is more interesting for us in this work – from the relationships in data being complex and non-linear. In the following sections, we focus on machine learning models, that can perform better in complex environments. Our main goal, as we mentioned before, is to explain the model of choice. Along self-explanatory linear models included as comparison, we considered models including: decision tree random forest gradient boosting xgboost. As mentioned earlier, the variables zipcode and waterfront were introduced as categorical. These models were built. Additionally, for the zipcode variable, which has 70 levels, we use one-hot encoding and build models again. The models are built in mlr (Bischl et al. 2016) R package with ranger (Wright and Ziegler 2015), gbm (Greenwell 2019), xgboost (Chen and Guestrin 2016) and rpart (Therneau 2019). For the models the RMSE score was calculated on the training and test set using the R auditor (Gosiewska and Biecek 2019b) package., the results were presented in the Figure 2.6. The smallest RMSE has a random forest model, xgboost, xgboost with one-hot encoding, and gbm with one-hot encoding. Other models have a much bigger error. So let’s take a look at these four models. FIGURE 2.6: Comparison of model performance by RMSE on training and test set. In the plot the trained models are marked with points, on the x-axis we have RMSE measure for the training set, on the y-axis the RMSE score for the test set, the line stands for RMSE equal on training and test sets Model results on the training and test set. Model RMSE train RMSE test random forest 0.07931109 0.1674559 xgboost 0.1076962 0.1600113 By analyzing the received models we obtained the following results. The random forest model is overfitted, while the xgboost model is a little better. On the test set they obtained similar scores. The Figure 2.7 shows the density of the residuals, for both models. We see that the xgboost model has larger residuals than the random forest model. FIGURE 2.7: Residuals density for random forest and xgboost models. Comparing the xgboost model with one-hot encoding and gbm with one-hot encoding we see that the RMSE score on the training and test set is very similar. Furthermore, the residual density plot (Figure 2.8 ) is practically identical. Model results on the training and test set with one-hot encoding. Model RMSE train RMSE test xgboost one-hot 0.1207626 0.1595913 gbm one-hot 0.1302585 0.1607376 FIGURE 2.8: Residuals density for gbm and xgboost models. 2.4 Explanations In this chapter we will present the methods of explainable machine learning models. They allow us to understand the compiled models. Following the plan from the Figure 2.1, we will show the explanations for the three groups (seller, buyer, and ad portal), but before we do that, we will continue the analytical approach to the regression problem and show how to apply XAI methods in model evaluation. We use Feature Importance to evaluate which variables are important in the model. This measure helps the Data scientists assess which variables have the greatest impact on prediction. Below in Figure 2.9 created with R ingredients (Biecek, Baniecki, and Izdebski 2019) package we have four models that have the best RMSE performance. Analyzing which variables are important in each model, we see that they all agree on the variables lat, which is latitude, long, which is longitude, m2_living, which is the square meters of the apartments interior living space, and grade, which is the level of construction and design. FIGURE 2.9: Comparison of the importance of the permutation variables for four models with the best RMSE score. Colour indicates the model, each bar shows the difference between the loss function for the original data (dashed line) and the permuted data for a particular variable, the longer the bar the greater the difference. The bars for each model start in a different position on the x-axis, this depends on the value of the loss function for the original data set. Now let’s look at the observation for which the xgboost model makes the biggest misprediction. FIGURE 2.10: Break Down Plot created with R iBreakDown (Gosiewska and Biecek 2019a) package for the observation whose prediction had the biggest error. The Break Down plot (Figure 2.10) shows the spread of the final prediction value over the variables and their contribution. The biggest contribution to this observation is grade and m2_living variables, their contribution is positive for price prediction. The value of the variable long has a negative impact on the value of the prediction. In order to take a closer look at this variable, we can plot Ceteris Paribus profiles (Figure 2.11). FIGURE 2.11: Ceteris Paribus profiles for the long variable, the line indicates the profiles for the observation, the x-axis indicates the value of the long variable, while the y-axis indicates the prediction for this observation when only the value of the long variable is changed, the dot indicates the value for the observation. Based on the Ceteris Paribus profile (Figure 2.11), we see that if the analyzed property was located to the west of its current location, the price prediction would increase. However, if the property was located to the east, the prediction would decrease. 2.5 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? References "],
["story-hotel-booking.html", "Chapter 3 Story hotel booking: eXplainable predictions of booking cancellation and guests coming back 3.1 Introduction 3.2 Model 3.3 Explanations 3.4 Summary and conclusions", " Chapter 3 Story hotel booking: eXplainable predictions of booking cancellation and guests coming back Authors: Domitrz Witalis (MIM), Seweryn Karolina (MiNI) Mentors: Jakub Tyrek (Data Scientist), Aleksander Pernach (Consultant) 3.1 Introduction The dataset is downloaded from the Kaggle competition website https://www.kaggle.com/jessemostipak/hotel-booking-demand. This dataset contains booking information for a city hotel and a resort hotel in Portugal, and includes information such as when the booking was made, length of stay, the number of adults, children, babies, the number of available parking spaces, chosen meals, price etc. There are 119 390 observations and 32 features. Below you can find features which were used in modelling. Furthermore, feature arrival_weekday was added. Feature Description 1 hotel Resort hotel or city hotel 2 is_canceled Value indicating if the booking was canceled (1) or not (0) 3 lead_time Number of days that elapsed between the reservation and the arrival date 4 arrival_date_month Month of arrival date 5 arrival_date_week_number Week number of year for arrival date 6 stays_in_weekend_nights Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel 7 stays_in_week_nights Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel 8 adults Number of adults 9 children Number of children 10 babies Number of babies 11 meal Type of meal booked 12 is_repeated_guest Value indicating if the booking name was from a repeated guest 13 previous_cancellations Number of previous bookings that were cancelled by the customer prior to the current booking 14 previous_bookings_not_canceled Number of previous bookings not cancelled by the customer prior to the current booking 15 booking_changes Number of changes made to the booking 16 deposit_type Indication on if the customer made a deposit to guarantee the booking. Three categories: No Deposit – no deposit was made; Non Refund – a deposit was made in the value of the total stay cost; Refundable – a deposit was made with a value under the total cost of stay 17 days_in_waiting_list Number of days the booking was in the waiting list before it was confirmed to the customer 18 adr Average Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights 19 required_car_parking_spaces Number of car parking spaces required by the customer 20 total_of_special_requests Number of special requests made by the customer (e.g. twin bed or high floor) 21 market_segment Market segment designation. 22 customer_type Contract - when the booking has an allotment or other type of contract associated to it; Group – when the booking is associated to a group; Transient – when the booking is not part of a group or contract, and is not associated to other transient booking; Transient-party – when the booking is transient, but is associated to at least other transient booking 23 distribution_channel Booking distribution channel. The booking website has information about these reservation characteristics and building models can help this company in better offer management. The most important information could be the prediction of booking cancellation, the prediction if client comes back to the hotel, the prediction whether client orders additional services (eg. meals), customer segmentation. In this project, we have decided to focus on two first issues. 3.1.1 Imbalanced dataset The distribution of the answer for the second problem is noticeably imbalanced (the ratio between number of observations with given answer is around 3%). We tested various methods, which are implemented in imbalanced-learn library, in different settings and found the RandomUnderSampler effective and sufficient for our needs as a data balancer for our main model in the second problem and RandomOverSampler as best balancer to use with simple SGDClassifier. The figure below presents the distribution of is_repeated_guest in the dataset. FIGURE 3.1: is_repeated_guest distribution 3.2 Model 3.2.1 Model 1. Booking cancellation The aim of this model is to predict whether guest cancels reservation and explanation of the reasons. The chosen model is XGBoost. Table below details the split of dataset. Train Test Number of observations 89542 29848 Number of events 33137 (37%) 11087 (37%) Bayesian optimisation with TPE tuner has been applied in order to improve model performance. Neural Network Intelligence (NNI) package has been chosen for this task, because it provides user-friendly GUI with summary of experiments. List of optimized hyperparameters and chosen values: max_depth - the maximum depth of tree (4). n_esimators - the number of trees (499). learning_rate - boosting learning rate (0.1). colsample_bytree - subsample ratio of columns when constructing each tree (0.78). Figure 3.2 below shows ROC curve of chosen model. The essential advantages of the model are high AUC and the lack of overfiting. FIGURE 3.2: XGBoost: ROC curve. In order to compare blackbox model with an interpretable model decision tree classifier was trained. It turned out that splits were made by features which are also important in blackbox model (xgboost). More details on this are given below. (#fig:roc_curve_tree)Decision Tree: ROC curve. (#fig:decision_tree)White box model of booking cancellation: decision tree. Let’s take one observation and analyze prediction of two models. We have chosen observation number 187. Both models predicts high probability of booking cancellation (Decision Tree:0.9940, Xgboost: 0.9982). FIGURE 3.3: The vector of feature values of chosen instance. (#fig:ex_breakdown)Break down plot explaining prediciton of chosen instance. (#fig:ex_lime)LIME plot explaining prediciton of chosen instance. We can see that explaination of XGBoost model says that features chosen in decision tree have also influence on prediction in XGBoost model. As shown illustrated in Figure @ref(fig:ex_cp) if chosen client had not canceled reservation in the past, he/she would be less likely to cancel this reservation. What is more, if the client had booked hotel later, he/she would have known his plans better and it would decrease probability of cancellation. Maybe the client canceled booking because of big family event, accident or breaking up with partner (booking for 2 adults). It is impossible to predict those events in advance. (#fig:ex_cp)Ceteris Paribus plot explaining prediciton of chosen instance. What is the lesson from this example? The performance of Decision Tree is worse than XGBoost, so if the explanation of blackbox model is intuitive it is better to use model with higher AUC. 3.2.2 Model 2. Repeated guests This model is meant to predict if the given guest is a repeating guest or not. For this purpose as our main model we chose the XGBClassifier from xgboost package. As mentioned above we have used RandomUnderSampler to balance the training dataset. When explaining various instances with the LIME explainer for one of the first models we noticed that the model highly relies on previous_bookings_not_canceled and previous_cancellations parameters. We decided to train a model without using those two variables to let the model focus on the other variables. The best models trained without previous_bookings_not_canceled variable had noticeably worse AUC score of 0.9 in comparison to 0.967 AUC achieved by our best models. Because of big influence on the model we decided to keep both variables. (#fig:ex_lime_2_1)LIME explanation for the first model. LIME explanation for the first model. As a result of the hyper parameter search we have found the optimal set of hyper parameters including: max_depth = 6, learning_rate = 0.33, n_estimators = 100, The model achieved AUC , and the figure above presents its ROC curve. We also trained two simpler models - SGDClassifier and DecisionTreeClassifier. While the SGDClassifier (which had the best performance with increased max_iter parameter and when using RandomOverSampler balancer) had significantly worse results than the XGBClassifier, the DecisionTreeClassifier achieved AUC score of 0.94 with the depth bounded by 4. We will focus on the SGDClassifier later, but for the sake of explanation we present the DecisionTreeClassifier tree here. (#fig:ex_lime_2_2)Plot of DecisionTreeClassifier model 3.3 Explanations 3.3.1 Model 1. Booking cancellation 3.3.1.1 Dataset level image Figure @ref(fig:feat_imp) presents feature importance. The list of five most important features contains deposit_type and previous_cancellations. Intuition suggests that these are important variables in such a problem. There are also variables required_car_parking_spaces, total_of_special_requests, market_segment that will be analyzed later. (#fig:shap_summary_plot)Summary of SHAP values of XGBoost model. Figure above shows SHAP values. There are some interesting findings which are intuitive: Clients who canceled some reservations in the past are more likely to cancel another reservation. People who buy refundable option cancel reservations more often than others. A lot of days between reservation time and arrival time increases probability of cancelling booking. The longer trip, the higher probability of cancellation. There are also less intuitive findings: Trip personalization (parking spaces, special requesrts) makes prediction of cancellation be lower. People without any special requests cancel reservetion more often than others. If trip starts at the end of the week there is higher probability that customers change their minds. The higher number of adults, the higher probability of cancellation. The probability of cancellation is lower if it is hotel in the city instead of resort hotel. 3.3.1.2 Instance level The lowest prediction of cancellation probability The prediction of probability of cancellation equals 0. The plot of SHAP values shows that client has booked 1 visit and has not canceled it. The values of features previous_cancelations=0 and previous_booking_not_canceled=1 make the probability of cancel be lower. The highest prediction of cancellation probability The prediction of probability of cancellation equals 1. In the past client canceled one reservation so it is more likely to cancel another one. 440 days between reservation and arrival date makes the probability of resignation be higher. It is intuitive, because the client could have changed plans. Price per night reduces prediction. The value of 75 euro per night is cheap compared to the prices in the dataset. We can guess that due to the low price, it may not be important for customers to cancel booking and wait for a refund. 3.3.2 Model 2. Repeated guests Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 3.3.2.1 Instance level explanations We first inspected the Shapley values for two interesting instances with different correct answer. The first instance showed us that the model learned that guests coming to the hotel in October are less likely to come back and that the lack of booking changes also affects repeating negatively. This explanations are reasonable, because in contrast to the holiday guests, the non-vacation time guests probably are visiting the hotel because of some other, independent reason, that is not as repeatable as the annual vacations. The similar reasoning can be repeated for the changes in the booking and number of special requests - when one comes to some place to relax, they will probably care more about additional attractions provided by the hotel and people who visit a relaxing place, when it met their expecations, probably will come again. The second guest, that is an adult coming to the hotel regularly (previous_booking_not_canceled) for a weekend (arrival_weekday and stays_in_week_nights) probably will come again for one more weekend, for the same reasons as they came before. (#fig:shap_2_1)Shapley values for non-repeating customer (#fig:shap_2_2)Shapley values for a repeating customer 3.3.2.1.1 Ceteris Paribus plot the same repeating guest From the Ceteris Paribus plot of lead_time variables for the same repeating guest as before we might get even more insight of the model’s reasoning. It clearly shows that the reservation made a year before the visit is an indicator that the guest will more likely come back. It might be a thing that this particular guest has some independent reason to visit the hotel regularly and they knows about it in advance, so because that reason probably is repeating, than they will probably visit the hotel once more. (#fig:cp_2_2)Ceteris Paribus plot of lead_time for a repeating customer The nonlinearity of the Ceteris Paribus profile of lead_time might be a clue why we were not able to achieve better results with a simple linear model. This result along with more similar ones may lead to effective feature engendering when focusing on creating less complex models. 3.3.2.2 Dataset level explanations The attempt to understand how important are particular variables for the trained model on the dataset level by calculating Permutational Variable Imporatance gave us a clear insight that the previous_booking_not_canceled variable is clearly the most important one, which is very reasonable, because the guest that have visited the hotel before will probably do it once more, in the future. (#fig:pci_2)Permutational variable importance od most important variables for XBBClassifier 3.4 Summary and conclusions Using XAI methods to examine trained models were useful to understand how the trained models work, and see that the explanations are reasonable enough to use the models, along with the explanations, as a great tool for the experts to give them some interesting insights about their customers behaviours. Moreover, even when explaining the complicated models we can get explanations that are easier to read and interpret than easier models, like a single decision tree. Last, but not least, when examining the models we were able to find a dependencies, that might be a partial reason for lower performance of other very simple, but easy to understand, linear model. "],
["story-uplift-modelling.html", "Chapter 4 Story Uplift Modelling: eXplaining colon cancer survival rate after treatment 4.1 Introduction 4.2 Data Preprocessing 4.3 Model 4.4 Explanations 4.5 Summary and Conclusions", " Chapter 4 Story Uplift Modelling: eXplaining colon cancer survival rate after treatment Authors: Aleksandra Łuczak (Warsaw University of Technology), Tymoteusz Makowski (Warsaw University of Technology), Kateryna Shulikova (Warsaw School of Economics) Mentors: Miłosz Dobersztyn (McKinsey), Armin Reinert (McKinsey) 4.1 Introduction We will use data about Chemotherapy for Stage B/C colon cancer from survival package in R. The documentation can be found here, the package can be installed with install.packages(\"survival\") command in R console. After the installation dataset can be accessed via survival::colon command. 4.1.1 What is Uplift Modelling? For classical algorithms in machine learning it is hard to predict causal impact of the event because they are more suited for predicting the results after an action. In some cases, such as a marketing campaign or medical treatment, that causal impact might be extremely important. Due to the possibility of using two training sets (treatment and control groups) by uplift modeling this problem was solved. Uplift modeling is one of the techniques or a branch of machine learning that tries to forecast class probability differences between group exposed to some action or therapy and control group (without that action or therapy). This technique also allows to discover in research those groups of patients for which treatment was most beneficial, so it is commonly used not only in marketing campaigns or medical treatments but also in other customer services. References: Uplift modeling with survival data (Jaroszewicz and Rzepakowski 2014), Uplift modeling for clinical trial data (Jaskowski and Jaroszewicz 2012), Uplift Modeling for Multiple Treatments with Cost Optimization (Zhao and Harinen 2019), Linear regression for uplift modeling (Krzysztof and Jaroszewicz 2018), Ensemble methods for uplift modeling (Sołtys, Jaroszewicz, and Rzepakowski 2015). 4.1.2 Dataset Description We use data from one of the first successful trials of adjuvant chemotherapy for colon cancer. There are two type of treatment: Levamisole is a low-toxicity compound previously used to treat worm infestations in animals and its effects on the treatment of colon cancer have been noted; 5-FluoroUracyl(FU) is a moderately toxic (as these things go) chemotherapy agent. This is the “strongest” one treatment. Both of these medications are given after the cancer excision, it’s adjutant chemistry, that means “extra post-operative”. There are two records per person, one for recurrence and one for death. Dataset contains 1858 observations and 16 features which are described in the 4.1 table. TABLE 4.1: Description of variables found in the dataset with values types from data exploration. Variable Type Description id categorical An id. study categorical 1 for all patients. rx categorical Treatment: Observation, Levamisole or Levamisole+5-FluoroUracyl. sex categorical Patient’s sex: Male or Female. age continuous Patient’s age in years. obstruct binary Stenosis of the colon by the cancer, which is blockage by the tumor. perfor binary Perforation of colon - a flag whether there was a hole in the colon. adhere binary Adherence to the surrounding organs (e.g. bladder). nodes continuous Number of lymph nodes with detectable cancer i.e. during the operation the lymph nodes that were attacked by the cancer are cut out. For the operation to be successful there should be at least 12 lymph nodes. time continuous Days until event or censoring. The time of receiving the treatment is considered to be time = 0, the time that passed in the variable time is the time until death or relapse from receiving the treatment. status binary Censoring status. differ categorical Differentiation of tumour cell (1=well, 2=moderate, 3=poor). The more the better because it is more like colon cells. extent categorical Extent of local spread, what cells did he reach (1=submucosa, 2=muscle, 3=serosa, 4=contiguous structures). The less the better. surg categorical Time from surgery to registration (0=short, 1=long). node4 binary More than 4 positive lymph nodes. etype categorical Event type: 1=recurrence, 2=death. Typically, the survival data includes two variables: the observed survival time (time variable in our dataset) and a binary censoring status variable (status variable in our dataset). The status variable indicates whether the event has been observed (typically denoted by status = 1). However, if the event has not been observed (status = 0) then the true survival time has ben censored, i.e. it is only known that the true survival time is at least equal to time (Jaroszewicz and Rzepakowski 2014). 4.1.3 Ideas There are many ways to use this data: prediction of the patient’s life expectancy depending on whether they received treatment or not, prediction whether the treatment is effective or not, prediction of the life expectancy depending on the medicine administered. We have decided to focus on the last mentioned approach. There are two approaches for modelling this approach. One of them being classification whether the patient will live longer than given threshold (Jaroszewicz and Rzepakowski 2014) and the second one, on whom we will focus, regression which will yield result by how much the treatment will change the life expectancy. 4.1.4 Why is it worth the hassle? When a patient learns about the colon cancer disease they usually ask “How much more time do I have left he has left?”. And now what? What is the treatment? The doctor may indicate a number of therapies that may be effective, but still be unable to tell how much time there is left or what’s the patient’s expectancy to live. The aim of this model is helping to provide more accurate data and answer the patient’s question and how the treatment is going to change their life expectancy. 4.2 Data Preprocessing We removed columns: id, study, etype, study - due to the intent of regression modelling of the problem. The dataset has been divided into: X - all features (without time and rx), y - target variable (time), treatment - rx variable. Distribution of the variable rx is as shown in the 4.2 table. This feature has been categorised. TABLE 4.2: Number of observations with given treatment type. Levamisole Levamisole + 5-FU Observation 620 608 630 4.3 Model To predict model we used algorithms from the package causalml in Python 3. To optimize hyper parameters we used algorithms from the package hyperopt also in Python 3. All notebooks and codes can be found on GitHub. The final model is XGBTRegressor with parameters summarised in the 4.3 table. TABLE 4.3: Final set of parameters used in our model. Parameter Value colsample_bytree 0.8336948571372381 gamma 0.5564260515876811 learning_rate 0.9327196556867555 max_depth 6 min_child_weight 0.45533158266464746 n_estimators 200 This gives as Average Treatment(Lev) Effect \\(74.38\\) and Average Treatment(Lev+5-FU) Effect \\(185.69\\). The average treatment effect (ATE) is a measure used to compare treatments in randomized experiments, evaluation of medical trials. The ATE measures the difference in mean outcomes between units assigned to the treatment and units assigned to the control. In a randomized trial the average treatment effect can be estimated from a sample using a comparison in mean outcomes for treated and untreated units. The treatment effect for individual \\(i\\) is given by \\(y_{1}(i)-y_{0}(i)=\\beta(i)\\). In the general case, there is no reason to expect this effect to be constant across individuals. The average treatment effect is given by the equation (4.1). \\[\\begin{equation} ATE = \\frac{1}{N}\\sum_{i}y_{1}(i)-y_{0}(i) \\tag{4.1} \\end{equation}\\] Where the sum in the (4.1) equation performed over all \\(N\\) individuals in the population. 4.4 Explanations 4.4.1 Dataset Level Explainations 4.4.2 Instance Level Explainations For the instance level explainations we have decided to focus on a limited number of observations. These observations are taken from set which seemed to provide interesting results during the data exploration process. The two observations which we have selected can be found in the 4.4 table. TABLE 4.4: The two observations selected from the dataset for instance level explainations. sex age obstruct perfor adhere nodes status differ extent surg node4 865 0 68 0 0 1 2 1 1 3 0 0 983 1 56 0 0 0 4 0 2 3 0 0 Both patients have the same severity of the cancer which is denoted by the extent variable. One of them is a female and the other is male. The older patient’s (age = 68) cancer is adhered to surrounding organs whilst the other’s cancer is not — this is denoted by the adhere variable. Finally, the last difference is in differ variable which is difference between colon cells and cancer cells. The Explanations plots have been created for both selected observations. For each of the observations there are two different plots. The reason for two different plots is the fact that XGBTRegressor model underneath creates a model for every treatment. Hence we have got one plot for every treatment type there is – in our case Levamisole(Lev) and Levamisole+5-FluoroUracyl(Lev+5FU) – as seen on figures below. 4.4.2.1 Break Down On the figures FIGURE 4.1: Break Down plots for patient with id 865. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. FIGURE 4.2: Break Down plots for patient with id 983. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. 4.4.2.2 LIME FIGURE 4.3: Lime plots for patient with id 865. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. FIGURE 4.4: Lime plots for patient with id 983. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. 4.4.2.3 SHAP The SHAP plots have been created for both selected observations. For each of the observations there are two different plots. The reason for two different plots is the fact that XGBTRegressor model underneath creates a model for every treatment. Hence we have got one plot for every treatment type there is – in our case Lev and Lev+5FU – as seen on figures 4.5 and 4.6. FIGURE 4.5: SHAP plots for patient with id 865. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. FIGURE 4.6: SHAP plots for patient with id 983. The left plot represents model using Levamisole treatment and the right one represents model using Levamisole+5-FluoroUracyl treatment. 4.5 Summary and Conclusions References "],
["story-meps-explainable-predictions-for-healthcare-expenditures.html", "Chapter 5 Story Meps: eXplainable predictions for healthcare expenditures 5.1 Introduction 5.2 Model 5.3 Explanations 5.4 Summary and conclusions", " Chapter 5 Story Meps: eXplainable predictions for healthcare expenditures Authors: Anna Kozioł (Warsaw University of Technology), Katarzyna Lorenc (Warsaw University of Technology), Piotr Podolski (University of Warsaw) Mentors: Maciej Andrzejak (Affiliation 2), Alicja Jośko (Affiliation 1) 5.1 Introduction Perhaps the most urgent problem with the current health care system in the United States is its high cost. According to the Centers for Disease Control and Prevention, during 2017 health care spending per capita averaged nearly $11,000 and total spending was $3.2 trillion, or 17.9% of GDP. This raises the natural question of the causality of high expenses and the estimation of them for a particular person. One of the objectives of this chapter is to forecast annual spending on the health care of individuals in the United States. There is no doubt that these forecasts are of interest to people directly related to medical expenditure, for example, insurance companies, employers, government. How to deal with a situation when the model works well but is a so-called black box and we do not know what affects a specific result? What if the proposed models return non-intuitive results and we want to know why they are wrong? The next and main purpose of this chapter is to address these concerns using Explanatory Model Analysis. We will try to identify not only which features are most predictable for the results, but also the nature of the relationship (e.g. its direction and shape). We will focus on understanding the behavior of the model as a whole, as well as in a specific instant level (for specific person). The data set comes from a study called Medical Expenditure Panel Survey (MEPS), which is sponsored by the Healthcare Quality and Research Agency. About 15,000 households are selected as a new panel of surveyed units, regularly since 1996. Data set used for analysis is available for free on the MEPS website. The MEPS contains a representative sample of the population from the United States with two major components: the Household Component and the Insurance Component. Household Component collects data about demographic characteristics, health conditions, health status, medical history, fees and sources of payment, access to care, satisfaction with care, health insurance coverage, income, and employment for each person surveyed. The second component - insurance - collects data about the health insurance from private and public sector employers. The data include the number and types of private insurance schemes offered, premiums, employers’ and employees’ health insurance contributions, benefits associated with these schemes, and employer characteristics. The data processing and analysis were carried out in Python 3.7.3 and R 3.6.1. 5.2 Model 5.2.1 Data Agency of Healthcare Research and Quality provides an extensive database of medical expenses. Consequently, dataset selection on which we will make further analysis was an important first step. We decided to choose the two latest panels. Expenditures for treatment that we will examine in the following chapter apply to the years 2015/2016 and 2016/2017. The selected dataset contains information on over 32,000 patients, and each of them is described by 3,700 variables. We attached great importance to choosing features that would be appropriate for the prediction. The most important criterion adopted is that the variable cannot relate to expenditure associated with any treatment. For this purpose, we looked through several hundred of them and selected 387 most suitable. As a part of the preprocessing, we removed records that were marked as Inapplicable in the expenditure column. The number of people who didn’t incur expenses is 5504, while the number of patients with “inapplicable” is 407, the percentage respectively are 17% and 1%. The following figures show the distribution of the explained variable. FIGURE 5.1: Distribution of medical expenses 5.2.2 Model Among the models we have trained, the best results were achieved by Gradient Boosting. Due to the characteristics of the explained variable, we decided to check the behavior of the model after applying the logarithmic transformation to expenses. We also checked whether the not inclusion of people without medical expenses would affect the model. Model hyperparameters have been tuned using NNI toolkit. To choose the best model, we compared the determination coefficient values. The table below shows the results of the experiments. To calculate the determination coefficient in column \\(R^2\\) (logarithmically transformed expenses), we transformed logarithmically the values of expenses, and after training the model we returned to the original scale. Values of the determination coefficient. Patients without expenses Model \\(R^2\\) \\(R^2\\) (logarithmically transformed expenses) included Gradient Boosting 0.50 - included Tuned Gradient Boosting 0.55 - not included Gradient Boosting 0.42 0.40 not included Tuned Gradient Boosting 0.49 0.49 The best fit relying on the determination coefficient was demonstrated by a Gradient Boosting, which included patients who did not incur treatment expenses. Then, as a compromise between the size of the model and its quality, we chose the 7 most important variables. For this purpose, we ranked the significance of the variables in the model and extracted those with the highest coefficient. Obtained variables mainly concern the number of visits to specialists. For a more diverse and interesting analysis, we have also taken into account demographic variables such as age, gender, educational background, and race, as well as some disease units. FIGURE 5.2: Scheme of conduct with a specification of origin of variables A review of selected variables Variable Description IPNGTDY1 number of nights associated with hospital discharges OBDRVY1 number of office-based physician visits HHAGDY1 agency home health provider days DSFTNV5 indicate whether the respondent reported having his or her feet checked for sores or irritations OBOTHVY1 office-based non-physican visits PSTATS2 person disposition status OPOTHVY1 outpatient dept non-dr visits AGE2X age of patient RACEV2X race of patient SEX patient’s gender HIDEG the highest degree of education attained at the time the individual entered MEPS diab_disease indicates whether the patient suffered from a diabetes disease art_disease indicates whether the patient suffered from a arthritis disease ast_disease indicates whether the patient suffered from a asthma disease press_disease indicates whether the patient suffered from a high pressure disease heart_disease indicates whether the patient suffered from a heart disease In the following section we will explain the Gradient Boosting model based on 16 variables presented in the table above. The coefficient of determination of the final model is 0.5 TODO: zaktualizowac wartosci R^2 5.3 Explanations 5.3.1 Model Level Explainations In order to find out about the influence of individual variables on the prediction for each patient, we present a Permutation Variable Importances graph. FIGURE 5.3: Permutation Variable Importances for Gradient Boosting Regressor Undoubtedly, the most important is the variable that indicates the number of nights spent in the hospital by the patient. An interesting observation seems to us that the demographic variable - AGE, which initially did not have a significant impact on the prediction, on the reduced model is in the top five most important variables. The remaining demographic variables, as well as those relating to diseases, do not show a gain in relevance in a model reduced to several variables. Based on previous analyzes, the number of nights spent in the hospital turned out to be the most important variable. To understand the nature of its impact on prediction in our model, it’s worth looking at the Partial Dependence Profiles. Below we present the PD plots broken down by gender. FIGURE 5.4: Partial Dependence Profiles for number of nights spent in the hospital broken down by gender Among patients who spent a few nights in the hospital, gender is not important for the amount of prediction. This rule begins to change after exceeding 30 nights. The PD profile for men has significantly higher values compared to the women’s profile, despite the similar curve behavior. After exceeding 70 nights in the hospital, this variable does not affect the result returned by the model on average. 5.3.2 Instance Level Explainations - business approach In this subsection we will try to show the application of explanatory methods in the business approach. Selected observations are: 1) the person with the best estimated cost among people with results greater than 3000, 2) the person for whom the model predicted the highest cost of all. Finding the value of characteristics that increase or decrease the final result, diagnostics of the direction of changes or oscillations of the result in case of change of characteristics describing a person may be valuable information for insurance companies or other payers for medical services. Such conclusions may also be useful for the patients themselves, who have decided to pay for medical care themselves. 5.3.2.1 XIA for the best prediction Prediction of medical costs for first observation is 3882$ and is differed from the real value by 4.8$. 5.3.2.2 XIA for the best prediction using Break Down Plots Break-down plots show how the contribution of individual variables change the average model prediction to the prediction for observation. FIGURE 5.5: Brake down plot The patient has 58 age, which alone increase average total cost by 4303.195 $ and the gender is female which decreases average total cost by 140. 72 $. Her total number of office-based visits is 4, which increase average total cost by 574.177 $. She suffers from arthritis what increase average total cost by 677.868 $ but she is not diagnose to diabets, astma or high blood preasure which decrease final result. Her status of education is unknown, what increase total cost. The fact that she didn’t spend any night in the hospital decrease average total cost by 1343.271$. And also, she didn’t benefit from home medical services decrease average total cost by 386.772 $. 5.3.2.3 XIA for the best prediction using Shapley Values To remove the influence of the random ordering of the variables in brake down results we can compute an average value of the contributions. FIGURE 5.6: Shapley values plot The plot shows that the most important variables, from the point of view of selected observation is age, number of night spending in the hospital and diagnose high blood preasure. For this obserwation having age equals 58 decrease average total cost by 116.435 $. Similar impact (decrease result by 116.25 $ ) have the fact that she didn’t spend any night in the hospital. Her status of education is unknown, what decrease average total cost by 104.521 $. She is not diagnose the high blood preasure which increase average response by 113.134 $. For this obserwation being a women increase average total cost by 70.099 $. 5.3.2.4 XIA for the best prediction using LIME TODO: poprawic wykresy lime The key idea behind this method is to locally approximate a black-box Gradient-Boosting model by a K-lasso interpretable model. FIGURE 5.7: LIME plot According to the LIME method, the plot suggests that spending any night in hospital reduces the estimated cost by $663. Much greater, also the negative impact has a variable which telling that that patient didn’t benefit from home medical services, which total cost by 16534.29 $. Patient analysed is not diagnose the high blood preasure which decrease response by 1308.27 $. Variables that increase the cost of medical services are the total number of office-based visits greater than 3 and the age of the analyzed person greater than 54. 5.3.2.5 XIA for for the best prediction Prediction of medical costs for second observation is 147178.5 $ and is differed from the real value by 3721.5$. 5.3.2.6 XIA for the prediction with the largest cost using Break Down Plots FIGURE 5.8: LIME plot The patient spending 52 nights in hospital, which increase average total cost by 4304.195 $. Having 59 age, increase average total cost by 16986 $ and the fact that gender is male increase average total cost by 15105 $. His status of education is bechelor degree, what increase total cost 1887 $. Despite he is not diagnose to diabets and high blood preasure which decrease final result, he suffers from heart disease which also decrese avarage response. 5.3.2.7 XIA for the prediction with the largest cost using Shapley Values FIGURE 5.9: Shapley Values plot As we expected, for the observation that generated the highest predictions, most of the variables have an additive effect on the final result. The graph shows that the greatest influence on the final value of the average prediction was in being male. Living in a household increases the average prediction by 18031$. The fact of having heart disease, spending 52 nights in hospital and the total number of office-based visits equal to 6 also had a big positive impact on the result. 5.3.2.8 XIA for the prediction with the largest cost using LIME FIGURE 5.10: LIME plot The LIME method also returns a positive influence on the final prediction for most variables. The chart shows that spending 52 nights in hospital increases treatment costs by 32360 $. The total number of office-based visits greater than 3 also has a positive impact, increasing the prediction by 6074.66 $. The age of a patient over 54 also significantly increases medical costs. Among the variables reducing treatment costs ,was the total number of days in home health care equal to 0. 5.3.2.9 XIA for both predictions using Ceteris Paribus Profiles Here we will denote the patient with the largest cost predicted as Patient 2, and the patient with the best prediction as Patient 1. FIGURE 5.11: Ceters Paribus plot for selected 6 features In the plot above we have selected features that behave differently between those two patients. In later subsections we will dive more into those differences. 5.3.2.9.1 Comparing differences between patients based on Sex. FIGURE 5.12: Ceters Paribus plot for SEX As we can see, our explanation model tries to predict what would have happened if the patient would have a different sex And for the patient 1 there would be no difference in predicted value, but for patient 2 the change of sex implicates lowering or increase in predicted costs. 5.3.2.9.2 Comparing differences between patients based on age. FIGURE 5.13: Ceters Paribus plot for AGE In this section we will investigate age. For patient 1 there is no influence on age on the expenses, but for patient 2 there is an influence of this variable. For patient 2 being around age of 60 and below age of 10 implicates a rise in predicted costs. 5.3.2.9.3 Comparing differences between patients based on number of nights associated with hospital discharges. FIGURE 5.14: Ceters Paribus plot for IPNGTDY1 Here both patients behave similary, but with different sensitivity. Patient 2, because of higher costs is more sensitive in changes of the number of nights associated with hospital discharges. 5.3.3 Instance Level Explainations - instance specific approach Here I will try to show the results of explanatory analysis methods for selected instances of data, that is for specifically selected different people with different background, race, age and sex. A review of selected variables Variable Patient 1 Patient 2 IPNGTDY1 0 3 OBDRVY1 12 8 HHAGDY1 0 0 DSFTNV5 2 -1 OBOTHVY1 0 6 PSTATS2 11 11 OPOTHVY1 0 3 AGE2X 71 34 RACEV2X 2 1 SEX 1 2 HIDEG 1 1 diab_disease 1 0 art_disease 1 0 ast_disease 0 0 press_disease 1 0 heart_disease 1 0 real expenses 2263 16268 prediction 8779 24373 So as in table above we will investigate 2 patients, where one is of age 71 and with several illnesses, where the second one is of age of 34, different sex and without illnesses. 5.3.3.1 XIA for Patient 1 using Break Down Plots Here we will be showing explanations using Break-down plots and explain the contribution of individual variables on the prediction, The first patient is of age 71, which in comparison to previous explanations should have significant impact on the prediction but not in this case. Here the biggest influence on the prediction has variable OBDRVY1, which is the number of office-based physician visits. Also diabetes and indicator whether the respondent reported having his or her feet checked for sores or irritations have positive influence on the predicted expenses. But variables IPNGTDY1, OBOTHVY1 and HIDEG have most significantly negative influence on the prediction. 5.3.3.2 XIA for Patient 1 using Shapley Values We can run explanatory analysis using shaplay values for those patients. For Patient 1, all observations apart from age have positive influence on the predicted value. 5.3.3.3 XIA for Patient 1 using LIME TODO: poprawic wykresy lime 5.3.3.4 XIA for Patient 2 using Break Down Plots The second patient is of age 34. In this case the most significant influences on the prediciton have variables IPNGTDY1 and OPOTHVY1 which are responsible for almost 17k of expenses. Other variables that also influence the prediction are OBDRVY1 and OBOTHVY1. Other variables have very small or small but negative influence on the prediciton. #### XIA for Patient 2 using Shapley Values We can run explanatory analysis using shaplay values for those patients. For Patient 1, all observations apart have positive influence on the predicted value. 5.3.3.5 XIA for Patient 2 using LIME TODO: poprawic wykresy lime 5.3.3.6 XIA for Patient 1 and Patient 2 using Ceteris Paribus Profiles In this chapter we will use Ceteris-paribus profiles for instance level explanations. Ceteris-paribus profiles show how the model response would change if a single variable is changed. So here we will be checking how would the model prediction change, if we change only one property of the patient and how it influences our model 5.3.3.6.1 Comparing differences between model predictions for Patient 1 and 2 for diffenret properties Here we present a plot, on which we list all variables that are different between patients and we will try to investigate, how the model behavior changes for each patient, when their properties change. FIGURE 5.15: Ceters Paribus plot for six variables As shown on the plot above, each value of the patient property influences a bit differently the outcome of our model. Patient 2, who has higher expenses is more sensitive on changes of the values of his properties. 5.3.3.6.1.1 Comparing differences between patients for number of nights associated with hospital discharges FIGURE 5.16: Ceters Paribus plot for IPNGTDY1 For this property of our patients, we would like to show how the model responds, when values of the number of nights associated with hospital discharges influences the predicted costs. The number of nights influences costs for patient 1 and 2 similary, but with different size. Patient 2, due to having higher prediction of expenses is being influenced more, than patient 1, but with the same dynamic. 5.3.3.6.1.2 Comparing differences between patients for variable if feet was checked for sores or irritations FIGURE 5.17: Ceters Paribus plot for DSFTNV5 For the variable if of the feet of the patients was checked for sores or irritations only patient 2 shows some responce for any change of the property. 5.3.3.6.1.3 Comparing differences between patients for variable of number of office-based physician visits. FIGURE 5.18: Ceters Paribus plot for OBDRVY1 In this case we can notice an interesting influence of the variable OBDRVY1. For patient 1 the higher the number of office-based physician visits the higher would be the predicted outcome of our model. This is different for patient 2, for whom the predicted value is not changing, sometimes it is even declining. 5.4 Summary and conclusions All key information about the final model we will put in this section. Scripts and list of selected variables are available at: meps_story github. "],
["story-meps-healthcare-expenditures-of-individuals.html", "Chapter 6 Story MEPS: Healthcare expenditures of individuals 6.1 TODOS 6.2 Introduction 6.3 Models 6.4 Explanations 6.5 Summary and conclusions", " Chapter 6 Story MEPS: Healthcare expenditures of individuals Authors: Dominika Bankiewicz (University of Warsaw), Jakub Białek (Warsaw University of Technology), Agata Pładyn (Warsaw University of Technology) Mentors: 6.1 TODOS Opisy wszystkich zmiennych do appendixa (tabela?) Rozkład/percentyle Y, przed i po transformacji (wtedy łatwiej ocenić wyniki absolutne) do wstępu. Podrozdzział z wynikami każdego z modeli? W rozdziale modele - krótki wstęp, tam napisać, o transformacji y i o tym, że korzystaliśmy z tego samego train/test splitu. 6.2 Introduction In this part, models that predict annual healthcare expenditure of individuals will be developed and analyzed using XAI methods. The data set analyzed is called MEPS (Medical Expenditure Panel Survey) and it is freely accessible at [ref1]. The data comes from large-scale surveys of families, individuals, medical providers and employers from the United States of America. Each observation of the data set contains total expenditures of an individual as well as number of other variables describing his or her demographic and socio-economic status. This allows to create models predicting the expenditure based on other factors. For this reason it is particularly interesting from the point of view of the subject that is financially responsible for the healthcare cost – insurance company, government, healthcare provider or individuals. It is important to mention, that for all of these subjects, accurate model is only a part of the success – the other part are the relations between input parameters and predictions of the model. Having that we can provide an answer not only for how much? but also for why? Hopefully, this can be achieved with available XAI methods. The data analyzed in the following sections was not downloaded directly from MEPS website. Instead, it was obtained through IBM’s AIX360 [ref2]. Therefore it is initially preprocessed – race is restricted to black and white and ethnicity to non-Hispanic, records with special values (negative integers) are removed for some of the variables, variables are initially selected. The dataset provides 18350 observations and it contains variables that describe: demographics (age, gender, marital status), socio-economics (education, income, insurance coverage), self-reported health status (self-evaluation of physical and mental health status), diagnoses (stroke, cancer, heart disease, diabetes), health limitations (cognitive, activity, sensory). The following section describes the development of three different models for predicting the transformed total health expenditure of an individual. Once developed, these models are compared in terms of their quality and of them is selected and analyzed using XAI methods. Explanations provided by different methods are discussed. The baseline for this discussion was provided by the authors of IBM’s AIX360 [ref3]. 6.3 Models In the following subchapters the development of three different models is briefly described. Full details on the implementation together with the code itself can be found here. First of all, since the distribution of the predicted variable is strongly skewed, it was transformed with logarithm base 3. Typically in such cases, natural logarithm is chosen, but having in mind that impact of input variables on the prediction will be analyzed, the decision was made to use base 3 instead (when we see that some input variable affected the prediction increasing it by one, then we can say that it increased the total expenditures by the factor of three, not factor of Euler number). In order to ensure that results from all three models can be directly compared, models are trained and evaluated on the same, arbitrarily chosen, test and validation subsets. These can also be found at [repo]. The evaluation metrics are RMSE, MAE and R^2 [ref to the table with results]. 6.3.1 Model 1: Linear Linear model was created with sklearn package. Following operations were performed to prepare the data: logarithm change of the explainable variable (as described before) min and max scaling of variables RTHLTH31, MNHLTH31, POVCAT15. This variables describe the state of general health, mental health and poverty status in values (decoded to: poor, fair, good, very good, excellent). addition of a new column which counts for how many diseases/health issues patient was tested positive. The ridge regression was performed with GridSearchCV. This allowed to perform 5-fold cross-validation with a range of different regularization parameters in order to find the optimal value of alpha parameter. The final results of the ridge regression model are shown in a table (values rounded to 2 decimal points): RMSE MAE R^2 Train 2.27 1.72 0.33 Test 2.25 1.7 0.3 Figure presents the prediction values compare to real values of target variable on training dataset. Figure presents the prediction values compare to real values of target variable on test dataset. 6.3.2 Model 2: ANN The second model evaluated was a multilayer perceptron. The input data was preprocessed with use of scikit-learn [ref to sklearn] tools: numerical features were standardized categorical features were one-hot encoded The data was fed into ANN with 4 hidden, fully-connected layers. The ANN model itself was created with Keras [ref to Keras]. Alltogether, the preprocessing steps and the model, was wrapped into scikit-learn pipeline so it can be easily use with XAI methods. The results are shown in the table below: RMSE MAE R^2 Train 2.04 1.51 0.45 Test 2.17 1.61 0.37 6.3.3 Model 3: XGB XGB model was developed using scikit-learn package (scikit-learn 2019a). For this model, data was prepared in the following way: as was mentioned in Introduction, target variable was transformed with logarithm base 3, categorical features (i.e. features with 10 or less unique values, except variables “POVCAT15”, “RTHLTH31”, “MNHLTH31” which can be treated as continuous) were transformed with OneHotEncoder (scikit-learn 2019c), numerical features were transformed with StandardScaler (scikit-learn 2019d). Hiperparameters tuning was done with GridSearchCV (scikit-learn 2019b). Following parameters were optimized: n_esimators - the number of trees, max_depth - the maximum depth of tree, min_samples_split - the minimum number of samples required to split an internal node, min_samples_leaf - the minimum number of samples required to be at a leaf node. Results for best hyperparameters shows below table. RMSE MAE R^2 Train 2.04 1.51 0.45 Test 2.17 1.61 0.37 Figure presents the prediction values compare to real values of target variable on training dataset. Figure presents the prediction values compare to real values of target variable on test dataset. Finally, for XAI methods analysis, model was wrapped in pipeline. 6.3.4 Results Table below presents results for all developed models. RMSE (train) MAE (train) R^2 (train) RMSE (test) MAE (test) R^2 (test) LR 2.27 1.72 0.33 2.25 1.70 0.30 ANN 2.14 1.59 0.40 2.17 1.61 0.37 XGB 2.04 1.51 0.45 2.17 1.61 0.37 6.4 Explanations 6.4.1 Instance level 6.4.1.1 Observation with the best prediction 6.4.1.2 Observation with the worst prediction This patient it’s a 76 years old woman. She is a widow and has a GED or high school degree. She complains about poor health status and fair mental health status. She has ever been diagnosed with high blood pressure, coronary heart disease, emphysema, chronic bronchitis, high cholesterol, cancer, rheumatoid arthritis and joint pain last 12 months. She has social and cognitive limitation and limitation in physical functioning. She doesn’t smoke and has serious difficulty see or wears glasses. Her health insurance coverage indicator is public only. Total health expenditure of this woman is equal to zero but model predicted that it is equal to about 12005.68 (3 to the power of 8.55). Figure presents Break Down plot for observation selected in this subchapter Picture below shows Shapley Values for this observation. As we can see, the largest impact on total health expenditure of this woman is the fact that she has been diagnosed with cancer, high blood pressure and how old she is. Figure presents Shapley Values for observation selected in this subchapter Three plots below show the Ceteris Paribus profiles for this observation. They say how total health expenditure of this woman would change if a variable had a different value with the other variables unchanged. They complete the chart with Shapley Values. As you can see, the fact that this woman was diagnosed with cancer increases her spending by about 3 to the power of 0.5 times, while the fact that she was diagnosed with high blood pressure increases her expenditure by about 3 to the power of 0.3 times. If she were younger by at least about 10 years, expenses would be smaller, and their would be the smallest if she were about 35-50 years old. image image image 6.4.1.3 Observation with high prediction The patient with high expected health expenses value is a 66 years old women. She is married and have a bachelor degree. Her health expenses are equal to 36 295 (9.557 to the power of 3) and the models prediction is equal to 34 658 (3 to the power of 9.515). The patient suffers from many conditions including cancer, high blood pressure, angina, heart disease (had heart attack), high cholesterol, diabetes, joint pains. She has social and physical functioning limitations. She has a private insurance. Break Down explanations of model prediction for the patient. Break Down explanations indicates that the patient’s diseases have significant positive impact on the prediction. Shap explanations of model prediction for the patient. Ceteris Paribus explanations of model prediction for the patient. When investigated changes in the patients descriptions variables there are some additional interesting results. Despite patient positive diagnosis for many diseases, the health expenses would be lower if the patient age (Ceteris Paribus plot of the variable AGE31X) would be much more younger (20 years old or less). Patients walking limitations (Ceteris Paribus plot of the variable WLKLIM31) increases the prediction up to 1.73 (3 to the power of 0.5) times. Her low result in physical component summary (Ceteris Paribus plot for the variable PCS42) also increases the expenses cost (by up to 3 to the power of 1.5 times). Ceteris Paribus plot of the variable INSCOV15 shows the relation between type of insurance and the prediction. The patient has the private insurance (value 1) and if she had changed to public or opted for none she would have lower prediction. 6.4.1.4 Observation with low prediction The patient is a 21 years old man. He has never been married. He has only graduated from high school. He is poor. He has not have any insurance. He perceives his health as at excellent state. He smokes but he has not been diagnosed with any diseases. His health expenses are equal to 0.0. Model’s prediction is equal to 3 to the power of 0.004. Break Down explanations of model prediction for the patient. The Break Down explanations indicates his overall ratings of feeling may increase the prediction but after considering all the other factors the model prediction is equal almost to 0. Shap explanations of model prediction for the patient. Shap explanations shows that his not student status lowers the prediction by 3 to the power of 0.26 times. His good eyesight, no walking limitations and lack of heart diseases lowers the prediction 3 time to the power of around 0.2 times. Generally his lack of diseases lowers the predictions. Ceteris Paribus explanations of model prediction for the patient. If the patient had been diagnosed with diabetes his health expenses would be 3 to the power of 2.5 times higher (plot of the variable DIABDX). Positive diagnoses for high cholesterol would also increase by 3 times the health expenses prediction. What is more, he is not a student, what results in lower prediction by up to 3 to the power 1.3 times (plot of the variable FTSTU31X). His poverty status decreases the prediction by almost 3 times. 6.4.2 Dataset level In this part the focus is put on model-level explanations. Variables that are interesting, intuitive or simply important from the model perspective will be investigated. 6.4.2.1 Variable importance Figure presents permutational variable importance of ten most important features for XGB model. 6.4.2.2 Age variable Intuitively, the age of the patient should be a very good predictor. Even non-experts can tell that usually the older the person is, the more issues with health it has and thus - the more money will be spend on healthcare. Lets have a look at PDP and ALE profiles of this variable in our model: ALE and PDP profiles of age variable Both profiles follow similar trends and in general they show the expected behaviour. Some people are born with health issues already, so they generate high costs at the very beginning of their lives. Then, the ones who manage to overcome these initial issues are generally healthy. Usually teenagers are in a good shape - a lot of physical activity, careful parents, regular eating at schools etc. Once they become adults (16-18) their life becomes riskier (driving license, alcohol). Some of them also start to earn their own money. All of this sums up to sharp increase in medical expenditures of this group. Then there’s another plateau and another sharp increase at the age of about 50 years. This the age when a lot of disease are getting more probable thus people are taking clinical tests, diagnose and start to cure. This is also the age when menopause happens to women which usually worsens their health status. From now on, health status becomes worse and health expenditures are usually getting higher every year. From the model developer perspective - this relation seems to be a little raged. One would expect to be more smooth and monotonic. For example, there is no particular reason why medical expenditures should decrease at once we turn 60, but the plot shows otherwise. This suggests that it might be a good idea to discretize the age variable into number of groups. While there is very small probability that your health status will get worse next year, it is highly likely that it will get worse 10 years from now. It is worth investigation whether this transformation of the variable will improve the results. From the perspective of the medical provider and patient itself - we can clearly see that there are some points at which the expenditures sharply increase. It might be good idea for a patient to buy additional insurance once he or she approach that age. On the other hand, insurance provider should take that into account while preparing his offer. If the agreement time is couple of years and it includes that specific time when health sharply worsens, this should be included in price. 6.4.2.3 Evaluation of health status Evaluation of one’s health status seems to be an obvious indicator of what can we expect in terms of health expenditure. One should be cautious though - the methodology of this measurement is crucial here. Let’s have a look at profiles of variables PCS42 and RTHLTH31 - both are self-evaluation of ones health. PCS42 was created based on set of concrete questions about specific pain, limitations in activities etc. The higher the value, the better the health. On the other hand, RTHLTH31 just describes the overall health condition in one word, corresponding to the scale from 1 to 5 (“excellent”, “very good” “good” “fair” and “poor”). In this case, the higher the value, the worse the health. Both plots show the expected behaviour of the model. It is interesting to see though, that there is not much difference between “excellent” and “very good”. Similarly - the difference is very small between “fair” and “poor”. This is primarily caused by the subjective matter of these answers. First of all the understanding of the words their selves - something which is very good for one person, might be just good for other. Second thing is that we usually compare our current health to the recent past. If one feels tired every day, he might say that his health is fair. But if he feels a little tired every day but he just recovered from flu, he will say he feels very good, or excellent (in comparison to how he felt a week ago). Finally, people tend to get used to their diseases. It is proven, that people feel very bad at the beginning - when they are diagnosed, but when the time goes by they care less and less - they just get used to living with a disease. Nevertheless, it is important to say that insurance provideres should be very careful while pricing their services partially basing on surveys. It is crucial to ask specific questions that cannot be biased by subjective feelings of the respondent. 6.5 Summary and conclusions References "],
["story-lungs.html", "Chapter 7 Story lungs: eXplainable predictions for post operational risks 7.1 Introduction 7.2 Model 7.3 Explanations 7.4 Summary and conclusions", " Chapter 7 Story lungs: eXplainable predictions for post operational risks Authors: Maciej Bartczak (UW), Marika Partyka (PW) Mentors: Aleksandra Radziwiłł (McKinsey &amp; Company), Maciej Krasowski (McKinsey &amp; Company) 7.1 Introduction Science allows us to understand the world better. New technologies, data collection solves the problems not only of large companies but also of ordinary people. Especially if human life is at stake. They say that cancer is the killer of the 21st century. That’s why even small attempts to subdue this problem are important. In our work, we deal with lung cancer. We try to predict the chances of survival of a patient who has had a tumor removal surgery. We try different approaches. 7.2 Model The dataset consists of the following varaibles. Numerical Variable Unit date_birth date date_start_treatment date date_surgery date tumor_size_x cm tumor_size_y cm tumor_size_z cm years_smoking years age years time_to_surgery years (“today” - date_surgery) Categorical Variable Decription Values sex subject sex male/female histopatological_diagnosis type of cancer Rak płaskonabłonkowy, pleomorficzny, …* symptoms whether symptoms were observed yes/no lung_cancer_in_family whether family member had cancer yes/no stadium_uicc severity of tumor IA1, IA2, IA3, IB, IIA, IIB, IIIA, IIIB, IVA, IVB* alive whether subject is alive yes/no (target variable) About 1/3 af values of varaibles annotated with * was missing. Survivability was registered time_to_surgery years after the procedure, for the majority of the subjects not later than 1 year after the procedure, and at most after 12 years. We have tried out several models as well as different preprocessing strategies. However, all of the approaches yielded similar results differing no more than 0.01 of cross validation acuracy and 0.01 ROC AUC score. These are the models that were utilized: Logistic regression Logistic regression with hyperparameters cross validation Random Forest XGBoost Neural Net - 10 hidden neurons Neural Net - 30 hidden neurons Neural Net - 2 x 10 hidden neurons As we have indentified higle defendent features as well as observed that tumor sizes disturbed the explaination we have employed following preprocessing strategies: encode and normalize encode, remove highly dependent features and normalize encode, introduce tumor volume, discard tumor sizes, remove highly dependent features and normalize Finally we have settled on Neural Net with 10 hidden neurons with following receiver operating curve. 7.3 Explanations We are based on 3 methods of explaining models, mainly Ceteris Paribus as well as Shap and Variable Importance. Of course, it is our goal to understand on what basis our model has found a chance of survival of a given patient. For example, let’s take a patient with the following variable values: - date_start_treatment 2007-03-01 - sex M - histopatological_diagnosis rak niedrobnokomórkowy - years_smoking 35 - lung_cancer_in_family No - symptoms No - stadium_uicc IIIA - age 54 - time_to_surgery 0 - volume 125 Our most recent model indicated that the chances of survival after surgery for such a patient are \\(24\\%.\\) Here we have an explanation of this result by SHAP. According to our intuition, the lack of symptoms increases the chance of survival, quite advanced stage of UICC equal to IIIA reduces these chances. We may also notice that the prognosis is worse because our patient is a man. However, it will be best if we refer to the results of another method - Ceteris Paribus. Here we see 3 variables that show well how a parameter change works for or against the patient. This confirms that women have a better prognosis of survival after surgery, the symptoms of the disease do not herald the best, and also the younger we are, the better we are able to cope with convalescence. Here too, the model shows that if our patient’s cancer was classified as much lighter, his chances would increase. Let’s take a look at another method for the whole data set - Variable Importance. The most important one seems to be the variable that says when the patient started treatment. The second most important is the UICC stage. Interestingly, the period of time the patient smoked cigarettes does not affect the outcome too much. But using model explanations not only helps to explain the result of the prediction, it can also give a hint how to improve our model. When we built the model on all variables, the explanations allowed us to find correlations. Let’s compare the CeterisParibus results for the full model and the deleted dependent variables. In the picture above you can see that the influence of two correlated variables was distributed between them, but after leaving only one of these variables, the influence accumulated on it (picture below). At this stage we can already conclude that the techniques of model explanations are not only useful at the end of our journey. They can give us tips on how to transform data or which variables should be deleted. 7.4 Summary and conclusions XAI methods legitimised employed approach of pruning the dataset. XAI methods yielded explainations consistent with biological intuintion, what builds up trust in the model. As variety of modelling and preprocessing approaches resulted in similar predicitive performance we conclude there is not much more to squeeze out of the dataset. "],
["heloc-credits.html", "Chapter 8 Heloc credits 8.1 Introduction 8.2 Dataset 8.3 Model 8.4 Explanations 8.5 Application for clients 8.6 Summary and conclusion", " Chapter 8 Heloc credits Authors: Tomasz Kurzelewski (University of Warsaw), Tomasz Radzikowski (Warsaw University of Technology) Mentors: Marta Gajewska (McKinsey &amp; Company), Amadeusz Andrzejewski (?) (McKinsey &amp; Company) 8.1 Introduction A home equity line of credit, or HELOC, is a loan in which the lender agrees to lend a maximum amount within an agreed period (called a term), where the collateral is the borrower’s equity in his/her house (akin to a second mortgage). Because a home often is a consumer’s most valuable asset, many homeowners use home equity credit lines only for major items, such as education, home improvements, or medical bills, and choose not to use them for day-to-day expenses. Since amount of such credit is not small, banks carefully review financial situation of applicants. Utmost care is taken so the whole process is transparent and decision is easily explainable to the client. Because of that any automated process also has to be explainable, and in this XAI methods may be helpful. 8.2 Dataset Our dataset - Home Equity Line of Credit (HELOC) - originally cames from Explainable Machine Learning Challange organized by FICO company. The data contains anonymized credit applications of HELOC credit lines, which are a type of loan, collateralized by a customer’s property. There are 23 predictors in the dataset, which describe following features: * ExternalRiskEstimate - consolidated indicator of risk markers (equivalent of polish BIK’s rate) * MSinceOldestTradeOpen - number of months that have elapsed since first trade * MSinceMostRecentTradeOpen - number of months that have elapsed since last opened trade * AverageMInFile - average months in file * NumSatisfactoryTrades - number of satisfactory trades * NumTrades60Ever2DerogPubRec - number of trades which are more than 60 past due * NumTrades90Ever2DerogPubRec - number of trades which are more than 90 past due * PercentTradesNeverDelq - percent of trades, that were not delinquent * MSinceMostRecentDelq - number of months that have elapsed since last delinquent trade * MaxDelq2PublicRecLast12M - the longest delinquency period in last 12 months * MaxDelqEver - the longest delinquency period * NumTotalTrades - total number of trades * NumTradesOpeninLast12M - number of trades opened in last 12 months * PercentInstallTrades - percent of installments trades * MSinceMostRecentInqexcl7days - months since last inquiry (excluding last 7 days) * NumInqLast6M - number of inquiries in last 6 months * NumInqLast6Mexcl7days - number of inquiries in last 6 months (excluding last 7 days) * NetFractionRevolvingBurden - revolving balance divided by credit limit * NetFractionInstallBurden - installment balance divided by original loan amount * NumRevolvingTradesWBalance - number of revolving trades with balance * NumInstallTradesWBalance - number of installment trades with balance * NumBank2NatlTradesWHighUtilization - number of trades with high utilization ratio (credit utilization ratio - the amount of a credit card balance compared to the credit limit) * PercentTradesWBalance - percent of trades with balance Features containing data about delinquency are coded to numeric scale and missing values are labeled with negative integer number. The majority of features are monotonically decreasing or increasing. Dataset has ..xxx.. observations, ..xxx.. of its belongs to class ‘Good’, what means that clients repaid their HELOC account within 2 years, and ..xxx.. to class ‘Bad’. 8.3 Model Since credit decision takes into account many variables related to the customer’s financial situation, and many of them were not included into dataset, accuracy of built models was not as good as we would want. Best results available in papers are around 0.84 AUC, while our model based on XGBoost scored around 0.80 AUC. We experimented with a different models, such based on SVM, Random Forest and XGBoost. The last two gave best results, both around 0.80 AUC, but we chose XGBoost since available implementation allowed us to reinforce monoticity bounds arising from business interpretation of variables. It should provide us with model better describing decision process. We also experimented with variable ‘ExternalRiskEstimate’. This variable representing external credit scoring must be based on other variables from the dataset, and because of that many trained models were relying almost exclusively on this single variable, marginalising importance of other ones, and oversimplifying explanation. Such explanation wouldn’t be in any way meaningful to potential applicant. What’s more, ExternalRiskEstimate can be explained with other variables with mean absolute error 2.5. 8.4 Explanations SHAP values: [alt text][images/08_shap1.png] 8.4.1 Explanation for management Since management is more focused on overall financial situation, they will get more information from data based on whole dataset than individual instances. One of the most important information is which variable are most important in our model. To uncover it we can use Permutation Feature Importance. In our case it were NetFractionRevolvingBurden, AverageMInFile and PercentTradesNeverDelq. 8.5 Application for clients While working on this project we tried to use SHAP values to change the particular feature in the observation, what should increase or decrease probability of a positive credit decision. Results was surprisingly bad: in some cases changing the most important feauture in a reasonable range did not affect the output. In others, even a small change disrupted the whole SHAP model. [alt text][images/08_ERE_pc.png] 8.6 Summary and conclusion Our results show that explainable artificial intelligence could be helpful for banking industry and could provide a valuable explanaition for clients, what is necessary in many countries, including Poland. Unfrotunatelly there are some drawbacks of those techniques, what was shown in previous sections. Although SHAP values present current client’s situation, it is not possible to modify values of a certain feature causing a monotonical increase of probability of a positive credit decision. "],
["story-uplift-marketing1.html", "Chapter 9 Story Uplift modeling on marketing dataset: eXplainable predictions for optimized marketing campaigns 9.1 Introduction 9.2 Dataset 9.3 Model [wymaga dopracowania] 9.4 Explanations [wymaga dopracowania] 9.5 Summary and conclusions", " Chapter 9 Story Uplift modeling on marketing dataset: eXplainable predictions for optimized marketing campaigns Authors: Jan Ludziejewski (Warsaw University), Paulina Tomaszewska (Warsaw University of Technology), Andżelika Zalewska (Warsaw University of Technology) Mentors: Łukasz Frydrych (McKinsey), Łukasz Pająk (McKinsey) 9.1 Introduction Running bussiness is a challenge. It involves making a lot of decisions in order to maximize profits and cut down costs - finding the tradeoff is not a straight-forward task. Here comes Machine Learning and uplift models that can help in optimizing marketing costs. People often ask whether it makes sense to address marketing campaigns to all company’s customers. From one point of view by sending an offer we think the probability that the customer will buy our product is higher - in fact it is not always the cases (the matter will be described in details later). On the other hand, making large-scale campaign is costly. Therefore it would be good to know what is the return of investment. Above we presented the common sense arguments but how science addresses a question: “Is it true that by sending the marketing offer we only extend the chance for the customer to buy our product and therefore extend our profit?”. The issue was already investigated (Verbeke and Bravo 2017) and it was pointed out that customers of any company can be divided into 4 groups 9.1. FIGURE 9.1: Customer types taking into consideration their response to treatment The image matrix was created based on the customer decision to buy a product depending on the fact that they were addressed by marketing campaign or not. The action used for trigggering in customer the particular behaviour is called treatment. In the 4 groups we distinghuish 9.1 : the customers that irrespective of the fact that they experienced treatment or now that are going to buy a product (these are called “sure things”) the customers that irrespective of the fact that they experienced treatment or now that are NOT going to buy a product (“lost causes”) the customers that without being exposed to marketing campaing would NOT buy a product (“persuadables”) the customers that without being exposed to marketing campaing would buy a product but in case thay receive a marketing offer they resign (“sleeping dogs”) It can be than observed that in case of “lost causes” and “sure things” sending a marketing offer makes no impact therefore it doesn’t make sense to spend money on targeting these customers. As the company we should however pay more attention to the groups “persuadables” and “sleeping dogs”. In case of the first bearing the costs of marketing campaign will bring benefit. In case of the latter we not only spend money on targeting them but as the result we will also discourage them from buying the product therefore we as a company loose two times. The case of sleeping dogs can seem irrealistic, therefore we present an example. Let’s imagine there is a customer that subscriped our paid newsletter. He forgot that he pays each month fixed fee. He would continue paying unless a company sends him a discount offer. At this moment the customer realises that he doesn;t need and offer and unsubscripes. By understading the structure of the company’s customers, it can target its offer more effectively. 9.1.1 Approaches towards uplift modeling In (Akshay Kumar 2018) it was pointed out that the problem of deciding whether it is profitable to send an offer to particular customer can be tackled from two different perspectives: predictive response modelling (it is classical classification task where model assigns probability to each of the classes) and uplift modelling (where the “incremental” probability of purchase is modelled). The latter is more interesting but at the same time more challenging. Uplift modeling is a technique that helps to determine probability gain that the customer by getting the marketing materials will buy a product. The field is relatievely new. The two most commmon approaches are (Lee 2018): Two Model In this method there are build two classifiers. The one is trained on observations that received treatment (model_T1) and the second is trained on observations that didn’t receive a treatment (model_T0). Later the uplift for particular observations is calculated. If the observation experienced treatment then it is an input to the model_T1 and the probability that the customer will buy a product is calculated. Later the if condition is investigated meaning what would happen if the customer didn’t receive a treatment. In Ssuch case the treatment indicator in observation’s feature is changed to “zero”. Such modified record is an input to model_T0 that predicts the probability that such customer will buy a product. The uplift is calculated as difference between output of the model_T1 and model_T0. The higher the difference, the more profitable is addressing marketing campaign to particular customer. One Model The one model approach is similar conceptually to the Two model approach with such a difference that instead of building two classifiers only one is used. Therefore every observation is an input to the model that generates prediction. Later the indicator in the treatment column is changed into the negation and such vector is used as input to the model that once again output probability that the customer buy a product. The uplift is the difference of the two predicted probabilities. [przydałoby się stworzyć jakąś prostą grafikę tutaj] As the uplift modeling is an emenrging field there isn’t a list of good practices in terms of what classifier is better to use. In (Zaniewicz and Jaroszewicz 2013), the autors investigated application of SVM. But due to the fact that SVM requires precise, long lasting finetuning we decided to use xgboost (the architecture of our solution is described in details in section Model). 9.2 Dataset There is a scarcity of well-documented datasets dedicated to uplift modeling. Therefore the autors of (Rzepakowski and Jaroszewicz 2012) proposed to artificially modify available datasets in order to extract information about treatment. As the purpose of this story is to investigate XAI techniques in the domain of uplift modeling we decided to use real life dataset. We chose Kevin Hillstrom’s dataset from E-Mail Analytics And Data Mining Challenge (Hillstrom 2008).The dataset consists of 64000 records reflecting customers that last purchased within 12 months. As a treatment an e-mail campaign was addressed: 1/3 of customers were randomly chosen to receive an e-mail campaign featuring Mens merchandise 1/3 were randomly chosen to receive an e-mail campaign featuring Womens merchandise 1/3 were randomly chosen to not receive an e-mail campaign (“control group”) As an expected behaviour the following actions were determined: * visit the company’s website within 2 weeks after sending to the customers a marketing campaign * purchase a product from the website within 2 weeks after sending to the customers a marketing campaign In the challenge the task was to determine whether the Mens or Womens e-mail campaign was successful. In our task we reformulated the task and we want to answer the question whether any e-mail campaign was profitable for the company. The features about customers in the dataset are specified in fig 9.2: FIGURE 9.2: Customer features in the dataset There is also information about customer activity in the two weeks following delivery of the e-mail campaign (these can be interpreted as labels): Visit: 1/0 indicator, 1 = Customer visited website in the following two weeks. Conversion: 1/0 indicator, 1 = Customer purchased merchandise in the following two weeks. Spend: Actual dollars spent in the following two weeks. [zwiększę potem czcionkę na grafice] 9.2.1 Feature engineering It is largely imbalanced - there is only about 15% of positive cases in column Visit and x% in column Conversion. In such situation we decided to use column Visit as a label. As the number of column is small we decided to use one-hot encoding for transforming categorical variables instead of target encoding. 9.3 Model [wymaga dopracowania] There is not many packages dedicated to uplift modeling in python. We investigated the two: pylift (“Pylift Package - Documentation,” n.d.) and pyuplift. The latter enables usage of 4 types of models - one of those is Two Model approach. In pylift package there is the TransformedOutcome class that generate predictions. However, the model itself is not well described and uses XGBRegressor unserneath that is not intuitive. Fortunately the package offer also the class UpliftEval that allow uplift metric visualization. In the scene, we decided to create our own classifier (as in the One-Model approach) and use UpliftEval class from pylift model for metric evaluation. As the classifier we used fine-tuned XGBoost. In the figure below we show the cumulative gain chart for train and test sets. FIGURE 9.3: Cumulative gain chart: (left) train set, (right) test set [DODAĆ ZDANIE JAK CZYTAĆ TEN WYKRES - JAK ON POWSTAJE] It can be seen that our model is better than random choice but much worse than practical/theoretical maximum possible. It is also worse than the case without sleeping dogs. It is worth emphesising that our model didn’t experience overfitting as its quality on train and test sets are similar. [napisać gdzieś że miara accuracy jest tu niewłaściwa i poprzeć to dowodem liczbowym] 9.4 Explanations [wymaga dopracowania] The model is already created and the metric show that it brings additional value.Here comes the question whether the model is reliable, does it make the decision based on the features that are important form expert knowledge perspective. Such judgement can be done based on results of XAI tools. We decided to investigate model interpretability from the perspective of the 4 customer groups @(fig:4groups). Therefore we chose one representative customer from each group and analyse the model on instance-level. 9.4.1 Individual perspective In order to explain model output for particular customer we employed Shapley values (???). We benefit from additive feature attribution property of shapley values to model the uplift: UPLIFT=P(PUCHASE|TREATMENT=1) - P(PURCHASE|TREATMENT=0)) –&gt; SHAP(P(PUCHASE|TREATMENT=1)) - SHAP(P(PUCHASE|TREATMENT=0)) = SHAP (UPLIFT) This gives us great opportunity to evaluate these two vectors of Shapley values independently. For example if we use any tree-based model, we can make use of tree-based kernel for shapley value estimation (faster and better convergent) instead of modelling it directly as a black blox model returning difference between two regressors. Experimental results proved, that these two ways of calulcation are providing close estimations, with precision to numerical errors. Below we present Shapley values for the customer with the highest and the lowest uplift computed directly on uplift (without using its additivity) @(fig:upliftSHAP). FIGURE 9.4: Shapley values: (left) customer with the lowest uplift, (right) customer with the highest uplift [DODAĆ WNIOSKI] In a table @(tab:upliftTABLE) there is a comparison of Shapley values obtained using two methods for the customer with the lowest uplift. Column.name Uplift Diff recency -0.0059 -0.0057 history -0.2735 -0.2750 mens -0.0095 -0.0091 womens -0.0568 -0.0550 zip_code_Suburban -0.0005 -0.0022 zip_code_Rural -0.0325 -0.0322 zip_code_Urban -0.0011 -0.0009 newbie 0.0046 0.0040 channel_Phone 0.0007 0.0009 channel_Web 0.0014 0.0012 chennel_Multichannel -0.0006 -0.0007 segment 0.0005 0.0002 Experimental results proved, that these two ways of calulcation are providing close estimations, with precision to numerical errors [TRZEBA NAPISAĆ TEŻ O WPŁYWIE LOSOWOŚCI - LOSUJEMY SUBSET] Conclusions In case of our model there is no need to apply LIME as its main advantages - sparsity - is not important as we have few columns. 9.4.2 Data scientist perspective Unfortunately, its impossible to calculate directly Permutation Feature Importance, because of the previously mentioned problem with lack of full information in both cases: Will the client make the purchse after treatment, and will he without it. Because of having in disposal only historical data (not an oracle), we have only one of these two informations. However, we can make use of the previously computed shapley values of uplift to calculate the same value of permutational feature importance as an average of local shapley importance (defined in a permutational way itself, however calculated in a smarter manner, more in (Lundberg and Lee 2017). We decided to evaluate feature importance not from the well-known dataset-level but subset-level. We extracted from the dataset 3 groups: “sleeping dogs”, “persuadables” and “no impact” (this group is a merge of the groups: “sure things” and “lost causes”). The division was based on the predicted uplift. Sleeping dogs have negative uplift, “no impact” have uplift from zero to the defined epsilon and persuadables have uplift greater than epsilon. We decided to not take into consideration epsilon in case of sleeping dogs as we want to be more conservative. The worst thing the company can do is to discourage the customer from buying. FIGURE 9.5: Variable importance - “sleeping dogs” FIGURE 9.6: Variable importance - “no impact” FIGURE 9.7: Variable importance - “persuadables” Conclusions [W TEJ CZĘŚCI DODAMY JESZCZE PDP] 9.5 Summary and conclusions Using XAI for uplift modeling helps to understand its complex models better. The analysis goes beyond just assesing whether the model is reliable… it can help the executive to understand better the company customers - their behaviour without paying for some extra surveys to investigate their attitude towards the company. A vital part of our work was adjusting XAI techniques for the particularities of uplift modeling. We found out that thanks its additivity Shapley values are well suited for uplift modelling - we showed two methods of using it. We identified limitations of well-known Permutation Feature Importance in terms of explaining uplift modeling. It is caused by the fact that unlike in other supervised models here we do not have exactly labels. Therefore we used the generalization of Shapley values that converge to Permutation Feature Importance. We employed the analysis for the three gropus of customers based on the corresponding uplift. ………………………………….. Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? References "],
["examples.html", "Examples", " Examples "],
["story-covid19.html", "Chapter 10 Story covid19: eXplainable predictions for mortality 10.1 Figures and citations 10.2 Introduction 10.3 Model 10.4 Explanations 10.5 Summary and conclusions", " Chapter 10 Story covid19: eXplainable predictions for mortality Authors: Author 1 (University 1), Author 2 (University 2), Author 3 (University 3) Mentors: Mentor 1 (Affiliation 1), Mentor 2 (Affiliation 2) 10.1 Figures and citations Cite other materials this way: This book is created with (Xie 2015). Refer to figures or chatpers this way: In Figure 10.1 we show a CC BY-NA-SA logo. The next chapter is 7. FIGURE 10.1: Licence: Creative Commons Attribution NonCommercial ShareAlike 10.2 Introduction Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem. 10.3 Model Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 10.4 Explanations Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 10.5 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? References "],
["story-compas-recidivism-reloaded.html", "Chapter 11 Story COMPAS: recidivism reloaded 11.1 Introduction 11.2 Model 11.3 Explanations 11.4 Summary and conclusions", " Chapter 11 Story COMPAS: recidivism reloaded Authors: Author 1 (University 1), Author 2 (University 2), Author 3 (University 3) Mentors: Mentor 1 (Affiliation 1), Mentor 2 (Affiliation 2) 11.1 Introduction Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem. 11.2 Model Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 11.3 Explanations Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 11.4 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements This project is inspired by a fantastic book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich. We used the LIML project as cornerstone for this reopsitory. "],
["references.html", "References", " References Akshay Kumar, Rishabh Kumar. 2018. “Uplift Modeling : Predicting Incremental Gains.” 2018. http://cs229.stanford.edu/proj2018/report/296.pdf. Biecek, Przemyslaw, Hubert Baniecki, and Adam Izdebski. 2019. Ingredients: Effects and Importances of Model Ingredients. https://cran.r-project.org/web/packages/ingredients/index.html. Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. Chen, Tianqi, and Carlos Guestrin. 2016. “XGBoost: A Scalable Tree Boosting System.” CoRR abs/1603.02754. http://arxiv.org/abs/1603.02754. Esmukov, Kostya. 2020. “Python Geocoding Toolbox.” https://geopy.readthedocs.io/en/latest/#. Gosiewska, Alicja, and Przemyslaw Biecek. 2019a. “iBreakDown: Uncertainty of Model Explanations for Non-additive Predictive Models.” https://arxiv.org/abs/1903.11420v1. Gosiewska, Alicja, and Przemysław Biecek. 2019b. “auditor: an R Package for Model-Agnostic Visual Validation and Diagnostics.” The R Journal 11 (2): 85–98. https://doi.org/10.32614/RJ-2019-036. Greenwell, B., B. &amp; Boehmke. 2019. Gbm: Generalized Boosted Regression Models. https://cran.r-project.org/web/packages/gbm/index.html. Hillstrom, Kevin. 2008. “The Minethatdata E-Mail Analytics and Data Mining Challenge Dataset.” 2008. https://blog.minethatdata.com/2008/03/minethatdata-e-mail-analytics-and-data.html. Jaroszewicz, S., and P. Rzepakowski. 2014. “Uplift Modeling with Survival Data.” In ACM Sigkdd Workshop on Health Informatics (Hi-Kdd’14). New York City, USA. Jaskowski, Maciej, and Szymon Jaroszewicz. 2012. “Uplift Modeling for Clinical Trial Data.” In. Krzysztof, Rudaś, and Szymon Jaroszewicz. 2018. “Linear Regression for Uplift Modeling.” Data Min. Knowl. Discov. 32 (5): 1275–1305. Lee, Josh Xin Jie. 2018. “Simple Machine Learning Techniques to Improve Your Marketing Strategy: Demystifying Uplift Models.” 2018. https://medium.com/datadriveninvestor/simple-machine-learning-techniques-to-improve-your-marketing-strategy-demystifying-uplift-models-dc4fb3f927a2. Lundberg, Scott, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In. “Pylift Package - Documentation.” n.d. https://pylift.readthedocs.io/en/latest/. R Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rzepakowski, Piotr, and Szymon Jaroszewicz. 2012. “Decision Trees for Uplift Modeling with Single and Multiple Treatments.” Knowledge and Information Systems - KAIS 32 (August). https://doi.org/10.1007/s10115-011-0434-0. scikit-learn. 2019a. GradientBoostingRegressor Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html. ———. 2019b. GridSearchCV Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html. ———. 2019c. OneHotEncoder Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html. ———. 2019d. StandardScaler Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. Selim, H. 2009. “Determinants of House Prices in Turkey: Hedonic Regression Versus Artificial Neural Network.” Expert Systems with Applications 36: 2843–52. Sołtys, Michał, Szymon Jaroszewicz, and Piotr Rzepakowski. 2015. “Ensemble Methods for Uplift Modeling.” Data Mining and Knowledge Discovery 29 (November). https://doi.org/10.1007/s10618-014-0383-9. Therneau, B., T. &amp; Atkinson. 2019. Rpart: Recursive Partitioning and Regression Trees. https://cran.r-project.org/web/packages/rpart/index.html. Verbeke, W., and C. Bravo. 2017. Profit Driven Business Analytics: A Practitioner’s Guide to Transforming Big Data into Added Value. Wiley and Sas Business Series. Wiley. https://books.google.pl/books?id=NCA3DwAAQBAJ. Wright, Marvin N., and Andreas Ziegler. 2015. “Ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” https://doi.org/10.18637/jss.v077.i01. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. Zaniewicz, Lukasz, and Szymon Jaroszewicz. 2013. “Support Vector Machines for Uplift Modeling.” In 2013 Ieee 13th International Conference on Data Mining Workshops, 131–38. IEEE. Zhao, Zhenyu, and Totte Harinen. 2019. “Uplift Modeling for Multiple Treatments with Cost Optimization.” http://arxiv.org/abs/1908.05372. "]
]
